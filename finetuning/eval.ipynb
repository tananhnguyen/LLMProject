{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2afea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ddc3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"results/checkpoint-65160\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_path = \"llama-2-7b-chat-base.pt\"\n",
    "device_map = {\"\": 0}\n",
    "dataset_name = \"rajpurkar/squad_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6fa3236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.15s/it]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "# Reload and merge\n",
    "base_model = model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f813a0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/tokenizer/tokenizer_config.json',\n",
       " 'models/tokenizer/special_tokens_map.json',\n",
       " 'models/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"models/finetune_model.pt\")\n",
    "tokenizer.save_pretrained(\"models/tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76108761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9742"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free VRAM\n",
    "import gc\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del base_model\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd673027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.16s/it]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/tokenizer/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"models/finetune_model.pt\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533897f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_conversation(example):\n",
    "    context = example['context']\n",
    "    question = example['question']\n",
    "    user_text = f\"Context: {context} Question: {question}\"\n",
    "    answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "\n",
    "    reformatted = f\"<s>[INST] {user_text} [/INST] {answer} </s>\"\n",
    "    return {\"text\" : reformatted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7f5b1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5ad3e96b604f3c001a3ff68a',\n",
       " 'title': 'Normans',\n",
       " 'context': 'Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further south in Cilicia and the Taurus Mountains. A Norman named Oursel led a force of \"Franks\" into the upper Euphrates valley in northern Syria. From 1073 to 1074, 8,000 of the 20,000 troops of the Armenian general Philaretus Brachamius were Normans—formerly of Oursel—led by Raimbaud. They even lent their ethnicity to the name of their castle: Afranji, meaning \"Franks.\" The known trade between Amalfi and Antioch and between Bari and Tarsus may be related to the presence of Italo-Normans in those cities while Amalfi and Bari were under Norman rule in Italy.',\n",
       " 'question': 'Who did the Turks take up service with?',\n",
       " 'answers': {'text': [], 'answer_start': []},\n",
       " 'text': '<s>[INST] Context: Some Normans joined Turkish forces to aid in the destruction of the Armenians vassal-states of Sassoun and Taron in far eastern Anatolia. Later, many took up service with the Armenian state further south in Cilicia and the Taurus Mountains. A Norman named Oursel led a force of \"Franks\" into the upper Euphrates valley in northern Syria. From 1073 to 1074, 8,000 of the 20,000 troops of the Armenian general Philaretus Brachamius were Normans—formerly of Oursel—led by Raimbaud. They even lent their ethnicity to the name of their castle: Afranji, meaning \"Franks.\" The known trade between Amalfi and Antioch and between Bari and Tarsus may be related to the presence of Italo-Normans in those cities while Amalfi and Bari were under Norman rule in Italy. Question: Who did the Turks take up service with? [/INST]  </s>'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model with a sample question\n",
    "val_dataset = load_dataset(dataset_name, split='validation')\n",
    "val_dataset = val_dataset.select(range(100))\n",
    "val_transformed_dataset = val_dataset.map(transform_conversation)\n",
    "val_transformed_dataset = val_transformed_dataset.shuffle(seed=42)\n",
    "val_transformed_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5b20323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Context: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries. Question: What century did the Normans first gain their separate identity? [/INST] 10th\n",
      "Expected answer: 10th century\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for question answering\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "sample = val_transformed_dataset[18]\n",
    "\n",
    "sample_context = sample['context']\n",
    "sample_question = sample['question']\n",
    "input_text = f\"Context: {sample_context} Question: {sample_question}\"\n",
    "\n",
    "output = pipe(f\"[INST] {input_text} [/INST]\")\n",
    "output_text = output[0]['generated_text']\n",
    "print(output_text)\n",
    "print(f\"Expected answer: {sample['answers']['text'][0] if sample['answers']['text'] else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ff84d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   8%|▊         | 8/100 [00:00<00:08, 11.41it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating answers: 100%|██████████| 100/100 [00:13<00:00,  7.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(enumerate(val_transformed_dataset), total=len(val_transformed_dataset), desc=\"Generating answers\")\n",
    "answers = {}\n",
    "\n",
    "for i, sample in pbar:\n",
    "    sample_context = sample['context']\n",
    "    sample_question = sample['question']\n",
    "    input_text = f\"Context: {sample_context} Question: {sample_question}\"\n",
    "    output = pipe(f\"[INST] {input_text} [/INST]\")\n",
    "    output_text = output[0]['generated_text']\n",
    "    output_text = output_text.split(\"[/INST]\")[-1].strip()\n",
    "    answers[sample['id']] = output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9661de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def convert_hf_squad2_to_json(hf_dataset, version=\"v2.0\"):\n",
    "    \"\"\"\n",
    "    Converts a Hugging Face SQuAD v2.0 dataset (or subset) to official SQuAD 2.0 JSON format.\n",
    "    Args:\n",
    "        hf_dataset: HuggingFace Dataset (e.g., dataset[\"train\"] or a sampled subset)\n",
    "        output_path: Path to save the JSON file\n",
    "        version: SQuAD version string (default \"v2.0\")\n",
    "    \"\"\"\n",
    "    def format_answers(answers):\n",
    "        # Converts {'text': [...], 'answer_start': [...]} to list of dicts\n",
    "        return [\n",
    "            {\"text\": t, \"answer_start\": s}\n",
    "            for t, s in zip(answers[\"text\"], answers[\"answer_start\"])\n",
    "        ] if answers and \"text\" in answers and \"answer_start\" in answers else []\n",
    "\n",
    "    data_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for ex in hf_dataset:\n",
    "        print(ex['id'])\n",
    "        title = ex.get(\"title\", \"No Title\")\n",
    "        context = ex[\"context\"]\n",
    "        qas_entry = {\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"is_impossible\": ex.get(\"is_impossible\", False),\n",
    "            \"answers\": format_answers(ex[\"answers\"])\n",
    "        }\n",
    "        if ex.get(\"is_impossible\", False):\n",
    "            plausible_answers = ex.get(\"plausible_answers\")\n",
    "            if plausible_answers:\n",
    "                qas_entry[\"plausible_answers\"] = format_answers(plausible_answers)\n",
    "            else:\n",
    "                qas_entry[\"plausible_answers\"] = []\n",
    "        data_dict[title][context].append(qas_entry)\n",
    "\n",
    "    data = []\n",
    "    for title, paras in data_dict.items():\n",
    "        paragraphs = []\n",
    "        for context, qas_list in paras.items():\n",
    "            paragraphs.append({\n",
    "                \"context\": context,\n",
    "                \"qas\": qas_list\n",
    "            })\n",
    "        data.append({\n",
    "            \"title\": title,\n",
    "            \"paragraphs\": paragraphs\n",
    "        })\n",
    "\n",
    "    squad_json = {\n",
    "        \"version\": version,\n",
    "        \"data\": data\n",
    "    }\n",
    "    return squad_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d71ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56ddde6b9a695914005b9628\n",
      "56ddde6b9a695914005b9629\n",
      "56ddde6b9a695914005b962a\n",
      "56ddde6b9a695914005b962b\n",
      "56ddde6b9a695914005b962c\n",
      "5ad39d53604f3c001a3fe8d1\n",
      "5ad39d53604f3c001a3fe8d2\n",
      "5ad39d53604f3c001a3fe8d3\n",
      "5ad39d53604f3c001a3fe8d4\n",
      "56dddf4066d3e219004dad5f\n",
      "56dddf4066d3e219004dad60\n",
      "56dddf4066d3e219004dad61\n",
      "5ad3a266604f3c001a3fea27\n",
      "5ad3a266604f3c001a3fea28\n",
      "5ad3a266604f3c001a3fea29\n",
      "5ad3a266604f3c001a3fea2a\n",
      "5ad3a266604f3c001a3fea2b\n",
      "56dde0379a695914005b9636\n",
      "56dde0379a695914005b9637\n",
      "5ad3ab70604f3c001a3feb89\n",
      "5ad3ab70604f3c001a3feb8a\n",
      "56dde0ba66d3e219004dad75\n",
      "56dde0ba66d3e219004dad76\n",
      "56dde0ba66d3e219004dad77\n",
      "5ad3ad61604f3c001a3fec0d\n",
      "5ad3ad61604f3c001a3fec0e\n",
      "5ad3ad61604f3c001a3fec0f\n",
      "5ad3ad61604f3c001a3fec10\n",
      "56dde1d966d3e219004dad8d\n",
      "5ad3ae14604f3c001a3fec39\n",
      "5ad3ae14604f3c001a3fec3a\n",
      "56dde27d9a695914005b9651\n",
      "56dde27d9a695914005b9652\n",
      "5ad3af11604f3c001a3fec63\n",
      "5ad3af11604f3c001a3fec64\n",
      "5ad3af11604f3c001a3fec65\n",
      "56dde2fa66d3e219004dad9b\n",
      "5ad3c626604f3c001a3ff011\n",
      "5ad3c626604f3c001a3ff012\n",
      "5ad3c626604f3c001a3ff013\n",
      "56de0f6a4396321400ee257f\n",
      "5ad3dbc6604f3c001a3ff3e9\n",
      "5ad3dbc6604f3c001a3ff3ea\n",
      "5ad3dbc6604f3c001a3ff3eb\n",
      "5ad3dbc6604f3c001a3ff3ec\n",
      "56de0ffd4396321400ee258d\n",
      "56de0ffd4396321400ee258e\n",
      "56de0ffd4396321400ee258f\n",
      "5ad3de8b604f3c001a3ff467\n",
      "5ad3de8b604f3c001a3ff468\n",
      "5ad3de8b604f3c001a3ff469\n",
      "5ad3de8b604f3c001a3ff46a\n",
      "56de10b44396321400ee2593\n",
      "56de10b44396321400ee2594\n",
      "56de10b44396321400ee2595\n",
      "5ad3e96b604f3c001a3ff689\n",
      "5ad3e96b604f3c001a3ff68a\n",
      "5ad3e96b604f3c001a3ff68b\n",
      "5ad3e96b604f3c001a3ff68c\n",
      "56de11154396321400ee25aa\n",
      "5ad3ea79604f3c001a3ff6e9\n",
      "5ad3ea79604f3c001a3ff6ea\n",
      "5ad3ea79604f3c001a3ff6eb\n",
      "56de148dcffd8e1900b4b5bc\n",
      "56de148dcffd8e1900b4b5bd\n",
      "56de148dcffd8e1900b4b5be\n",
      "5ad3ed26604f3c001a3ff799\n",
      "5ad3ed26604f3c001a3ff79a\n",
      "5ad3ed26604f3c001a3ff79b\n",
      "5ad3ed26604f3c001a3ff79c\n",
      "56de15104396321400ee25b7\n",
      "56de15104396321400ee25b8\n",
      "56de15104396321400ee25b9\n",
      "5ad3ee2d604f3c001a3ff7e1\n",
      "5ad3ee2d604f3c001a3ff7e2\n",
      "5ad3ee2d604f3c001a3ff7e3\n",
      "56de1563cffd8e1900b4b5c2\n",
      "56de1563cffd8e1900b4b5c3\n",
      "56de1563cffd8e1900b4b5c4\n",
      "5ad3f028604f3c001a3ff823\n",
      "5ad3f028604f3c001a3ff824\n",
      "5ad3f028604f3c001a3ff825\n",
      "56de15dbcffd8e1900b4b5c8\n",
      "56de15dbcffd8e1900b4b5c9\n",
      "56de15dbcffd8e1900b4b5ca\n",
      "56de15dbcffd8e1900b4b5cb\n",
      "5ad3f187604f3c001a3ff86f\n",
      "5ad3f187604f3c001a3ff870\n",
      "5ad3f187604f3c001a3ff871\n",
      "56de1645cffd8e1900b4b5d0\n",
      "56de1645cffd8e1900b4b5d1\n",
      "56de1645cffd8e1900b4b5d2\n",
      "5ad3f350604f3c001a3ff8ef\n",
      "5ad3f350604f3c001a3ff8f0\n",
      "5ad3f350604f3c001a3ff8f1\n",
      "56de16ca4396321400ee25c5\n",
      "56de16ca4396321400ee25c6\n",
      "56de16ca4396321400ee25c7\n",
      "56de16ca4396321400ee25c8\n",
      "5ad3f4b1604f3c001a3ff951\n"
     ]
    }
   ],
   "source": [
    "val_squad_form = convert_hf_squad2_to_json(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36dd5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"w\") as f:\n",
    "    json.dump(val_squad_form, f)\n",
    "\n",
    "with open(\"pred.json\", \"w\") as f:\n",
    "    json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5696417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!python sample_prediction_SQUAD2.0.py data.json pred.json --out-file eval.json --out-image-dir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00aec13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_subject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
