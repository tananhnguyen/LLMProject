{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 32940,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006071645415907711,
      "grad_norm": 17.296768188476562,
      "learning_rate": 1.9939283545840924e-05,
      "loss": 4.4972,
      "step": 100
    },
    {
      "epoch": 0.012143290831815421,
      "grad_norm": 13.71591854095459,
      "learning_rate": 1.987856709168185e-05,
      "loss": 2.9394,
      "step": 200
    },
    {
      "epoch": 0.018214936247723135,
      "grad_norm": 13.020685195922852,
      "learning_rate": 1.9817850637522772e-05,
      "loss": 2.5704,
      "step": 300
    },
    {
      "epoch": 0.024286581663630843,
      "grad_norm": 29.990793228149414,
      "learning_rate": 1.9757134183363694e-05,
      "loss": 2.4712,
      "step": 400
    },
    {
      "epoch": 0.030358227079538554,
      "grad_norm": 9.987337112426758,
      "learning_rate": 1.9696417729204616e-05,
      "loss": 2.3206,
      "step": 500
    },
    {
      "epoch": 0.03642987249544627,
      "grad_norm": 18.389711380004883,
      "learning_rate": 1.963570127504554e-05,
      "loss": 2.2644,
      "step": 600
    },
    {
      "epoch": 0.042501517911353974,
      "grad_norm": 30.271087646484375,
      "learning_rate": 1.957498482088646e-05,
      "loss": 2.1997,
      "step": 700
    },
    {
      "epoch": 0.048573163327261686,
      "grad_norm": 15.486847877502441,
      "learning_rate": 1.9514268366727383e-05,
      "loss": 2.0972,
      "step": 800
    },
    {
      "epoch": 0.0546448087431694,
      "grad_norm": 11.138538360595703,
      "learning_rate": 1.9453551912568305e-05,
      "loss": 1.9894,
      "step": 900
    },
    {
      "epoch": 0.06071645415907711,
      "grad_norm": 15.592857360839844,
      "learning_rate": 1.939283545840923e-05,
      "loss": 2.0844,
      "step": 1000
    },
    {
      "epoch": 0.06678809957498483,
      "grad_norm": 14.83871841430664,
      "learning_rate": 1.9332119004250153e-05,
      "loss": 2.0147,
      "step": 1100
    },
    {
      "epoch": 0.07285974499089254,
      "grad_norm": 15.89639663696289,
      "learning_rate": 1.9271402550091076e-05,
      "loss": 1.8708,
      "step": 1200
    },
    {
      "epoch": 0.07893139040680024,
      "grad_norm": 37.4422607421875,
      "learning_rate": 1.9210686095931998e-05,
      "loss": 1.8113,
      "step": 1300
    },
    {
      "epoch": 0.08500303582270795,
      "grad_norm": 23.993112564086914,
      "learning_rate": 1.9149969641772923e-05,
      "loss": 1.958,
      "step": 1400
    },
    {
      "epoch": 0.09107468123861566,
      "grad_norm": 19.340011596679688,
      "learning_rate": 1.9089253187613846e-05,
      "loss": 1.8538,
      "step": 1500
    },
    {
      "epoch": 0.09714632665452337,
      "grad_norm": 28.554101943969727,
      "learning_rate": 1.9028536733454768e-05,
      "loss": 1.8256,
      "step": 1600
    },
    {
      "epoch": 0.10321797207043108,
      "grad_norm": 20.931018829345703,
      "learning_rate": 1.896782027929569e-05,
      "loss": 1.8387,
      "step": 1700
    },
    {
      "epoch": 0.1092896174863388,
      "grad_norm": 16.60072135925293,
      "learning_rate": 1.8907103825136616e-05,
      "loss": 1.7069,
      "step": 1800
    },
    {
      "epoch": 0.1153612629022465,
      "grad_norm": 20.90943145751953,
      "learning_rate": 1.8846387370977538e-05,
      "loss": 1.803,
      "step": 1900
    },
    {
      "epoch": 0.12143290831815422,
      "grad_norm": 13.064579010009766,
      "learning_rate": 1.878567091681846e-05,
      "loss": 1.7229,
      "step": 2000
    },
    {
      "epoch": 0.12750455373406194,
      "grad_norm": 25.447494506835938,
      "learning_rate": 1.8724954462659383e-05,
      "loss": 1.7588,
      "step": 2100
    },
    {
      "epoch": 0.13357619914996965,
      "grad_norm": 31.68619728088379,
      "learning_rate": 1.8664238008500305e-05,
      "loss": 1.735,
      "step": 2200
    },
    {
      "epoch": 0.13964784456587737,
      "grad_norm": 25.392379760742188,
      "learning_rate": 1.8603521554341227e-05,
      "loss": 1.673,
      "step": 2300
    },
    {
      "epoch": 0.14571948998178508,
      "grad_norm": 18.82859992980957,
      "learning_rate": 1.854280510018215e-05,
      "loss": 1.6506,
      "step": 2400
    },
    {
      "epoch": 0.15179113539769276,
      "grad_norm": 18.23980140686035,
      "learning_rate": 1.848208864602307e-05,
      "loss": 1.7038,
      "step": 2500
    },
    {
      "epoch": 0.15786278081360047,
      "grad_norm": 17.03887176513672,
      "learning_rate": 1.8421372191863997e-05,
      "loss": 1.7405,
      "step": 2600
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 16.816585540771484,
      "learning_rate": 1.836065573770492e-05,
      "loss": 1.6187,
      "step": 2700
    },
    {
      "epoch": 0.1700060716454159,
      "grad_norm": 23.315940856933594,
      "learning_rate": 1.8299939283545842e-05,
      "loss": 1.6822,
      "step": 2800
    },
    {
      "epoch": 0.1760777170613236,
      "grad_norm": 15.140145301818848,
      "learning_rate": 1.8239222829386764e-05,
      "loss": 1.6146,
      "step": 2900
    },
    {
      "epoch": 0.18214936247723132,
      "grad_norm": 34.49775314331055,
      "learning_rate": 1.817850637522769e-05,
      "loss": 1.7205,
      "step": 3000
    },
    {
      "epoch": 0.18822100789313903,
      "grad_norm": 17.49134635925293,
      "learning_rate": 1.8117789921068612e-05,
      "loss": 1.639,
      "step": 3100
    },
    {
      "epoch": 0.19429265330904674,
      "grad_norm": 31.088600158691406,
      "learning_rate": 1.8057073466909534e-05,
      "loss": 1.6492,
      "step": 3200
    },
    {
      "epoch": 0.20036429872495445,
      "grad_norm": 26.98057746887207,
      "learning_rate": 1.7996357012750456e-05,
      "loss": 1.5417,
      "step": 3300
    },
    {
      "epoch": 0.20643594414086217,
      "grad_norm": 31.30488395690918,
      "learning_rate": 1.7935640558591382e-05,
      "loss": 1.5661,
      "step": 3400
    },
    {
      "epoch": 0.21250758955676988,
      "grad_norm": 18.076705932617188,
      "learning_rate": 1.7874924104432304e-05,
      "loss": 1.6492,
      "step": 3500
    },
    {
      "epoch": 0.2185792349726776,
      "grad_norm": 34.4478874206543,
      "learning_rate": 1.7814207650273227e-05,
      "loss": 1.6886,
      "step": 3600
    },
    {
      "epoch": 0.2246508803885853,
      "grad_norm": 29.395824432373047,
      "learning_rate": 1.775349119611415e-05,
      "loss": 1.6072,
      "step": 3700
    },
    {
      "epoch": 0.230722525804493,
      "grad_norm": 27.109176635742188,
      "learning_rate": 1.769277474195507e-05,
      "loss": 1.6565,
      "step": 3800
    },
    {
      "epoch": 0.23679417122040072,
      "grad_norm": 18.217073440551758,
      "learning_rate": 1.7632058287795993e-05,
      "loss": 1.5788,
      "step": 3900
    },
    {
      "epoch": 0.24286581663630843,
      "grad_norm": 16.345178604125977,
      "learning_rate": 1.7571341833636916e-05,
      "loss": 1.5608,
      "step": 4000
    },
    {
      "epoch": 0.24893746205221615,
      "grad_norm": 24.3499698638916,
      "learning_rate": 1.7510625379477838e-05,
      "loss": 1.558,
      "step": 4100
    },
    {
      "epoch": 0.2550091074681239,
      "grad_norm": 12.699803352355957,
      "learning_rate": 1.7449908925318764e-05,
      "loss": 1.5126,
      "step": 4200
    },
    {
      "epoch": 0.2610807528840316,
      "grad_norm": 34.03799819946289,
      "learning_rate": 1.7389192471159686e-05,
      "loss": 1.5304,
      "step": 4300
    },
    {
      "epoch": 0.2671523982999393,
      "grad_norm": 22.707347869873047,
      "learning_rate": 1.7328476017000608e-05,
      "loss": 1.5529,
      "step": 4400
    },
    {
      "epoch": 0.273224043715847,
      "grad_norm": 30.899436950683594,
      "learning_rate": 1.726775956284153e-05,
      "loss": 1.5499,
      "step": 4500
    },
    {
      "epoch": 0.27929568913175473,
      "grad_norm": 43.488075256347656,
      "learning_rate": 1.7207043108682456e-05,
      "loss": 1.5391,
      "step": 4600
    },
    {
      "epoch": 0.28536733454766244,
      "grad_norm": 17.153024673461914,
      "learning_rate": 1.7146326654523378e-05,
      "loss": 1.5705,
      "step": 4700
    },
    {
      "epoch": 0.29143897996357016,
      "grad_norm": 13.521018028259277,
      "learning_rate": 1.70856102003643e-05,
      "loss": 1.6025,
      "step": 4800
    },
    {
      "epoch": 0.2975106253794778,
      "grad_norm": 20.22541046142578,
      "learning_rate": 1.7024893746205223e-05,
      "loss": 1.4878,
      "step": 4900
    },
    {
      "epoch": 0.3035822707953855,
      "grad_norm": 30.84392547607422,
      "learning_rate": 1.696417729204615e-05,
      "loss": 1.5974,
      "step": 5000
    },
    {
      "epoch": 0.30965391621129323,
      "grad_norm": 21.9709415435791,
      "learning_rate": 1.690346083788707e-05,
      "loss": 1.6044,
      "step": 5100
    },
    {
      "epoch": 0.31572556162720095,
      "grad_norm": 20.33124542236328,
      "learning_rate": 1.6842744383727993e-05,
      "loss": 1.4918,
      "step": 5200
    },
    {
      "epoch": 0.32179720704310866,
      "grad_norm": 20.368267059326172,
      "learning_rate": 1.6782027929568915e-05,
      "loss": 1.5605,
      "step": 5300
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 27.52159881591797,
      "learning_rate": 1.6721311475409837e-05,
      "loss": 1.4536,
      "step": 5400
    },
    {
      "epoch": 0.3339404978749241,
      "grad_norm": 34.00455856323242,
      "learning_rate": 1.666059502125076e-05,
      "loss": 1.5018,
      "step": 5500
    },
    {
      "epoch": 0.3400121432908318,
      "grad_norm": 7.465768814086914,
      "learning_rate": 1.6599878567091682e-05,
      "loss": 1.5123,
      "step": 5600
    },
    {
      "epoch": 0.3460837887067395,
      "grad_norm": 21.148021697998047,
      "learning_rate": 1.6539162112932604e-05,
      "loss": 1.4958,
      "step": 5700
    },
    {
      "epoch": 0.3521554341226472,
      "grad_norm": 17.867237091064453,
      "learning_rate": 1.647844565877353e-05,
      "loss": 1.5542,
      "step": 5800
    },
    {
      "epoch": 0.3582270795385549,
      "grad_norm": 16.910526275634766,
      "learning_rate": 1.6417729204614452e-05,
      "loss": 1.5247,
      "step": 5900
    },
    {
      "epoch": 0.36429872495446264,
      "grad_norm": 28.750675201416016,
      "learning_rate": 1.6357012750455374e-05,
      "loss": 1.5195,
      "step": 6000
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 27.628952026367188,
      "learning_rate": 1.6296296296296297e-05,
      "loss": 1.5842,
      "step": 6100
    },
    {
      "epoch": 0.37644201578627806,
      "grad_norm": 35.35530090332031,
      "learning_rate": 1.6235579842137222e-05,
      "loss": 1.4549,
      "step": 6200
    },
    {
      "epoch": 0.3825136612021858,
      "grad_norm": 19.119380950927734,
      "learning_rate": 1.6174863387978145e-05,
      "loss": 1.4935,
      "step": 6300
    },
    {
      "epoch": 0.3885853066180935,
      "grad_norm": 24.729158401489258,
      "learning_rate": 1.6114146933819067e-05,
      "loss": 1.5583,
      "step": 6400
    },
    {
      "epoch": 0.3946569520340012,
      "grad_norm": 14.224651336669922,
      "learning_rate": 1.605343047965999e-05,
      "loss": 1.5411,
      "step": 6500
    },
    {
      "epoch": 0.4007285974499089,
      "grad_norm": 22.519134521484375,
      "learning_rate": 1.599271402550091e-05,
      "loss": 1.5275,
      "step": 6600
    },
    {
      "epoch": 0.4068002428658166,
      "grad_norm": 18.582857131958008,
      "learning_rate": 1.5931997571341834e-05,
      "loss": 1.4711,
      "step": 6700
    },
    {
      "epoch": 0.41287188828172433,
      "grad_norm": 35.604835510253906,
      "learning_rate": 1.5871281117182756e-05,
      "loss": 1.4516,
      "step": 6800
    },
    {
      "epoch": 0.41894353369763204,
      "grad_norm": 25.927825927734375,
      "learning_rate": 1.5810564663023678e-05,
      "loss": 1.3899,
      "step": 6900
    },
    {
      "epoch": 0.42501517911353975,
      "grad_norm": 27.476083755493164,
      "learning_rate": 1.5749848208864604e-05,
      "loss": 1.4291,
      "step": 7000
    },
    {
      "epoch": 0.43108682452944747,
      "grad_norm": 27.45785140991211,
      "learning_rate": 1.5689131754705526e-05,
      "loss": 1.5117,
      "step": 7100
    },
    {
      "epoch": 0.4371584699453552,
      "grad_norm": 25.33167266845703,
      "learning_rate": 1.5628415300546448e-05,
      "loss": 1.4305,
      "step": 7200
    },
    {
      "epoch": 0.4432301153612629,
      "grad_norm": 28.904844284057617,
      "learning_rate": 1.556769884638737e-05,
      "loss": 1.4193,
      "step": 7300
    },
    {
      "epoch": 0.4493017607771706,
      "grad_norm": 28.714426040649414,
      "learning_rate": 1.5506982392228296e-05,
      "loss": 1.4477,
      "step": 7400
    },
    {
      "epoch": 0.4553734061930783,
      "grad_norm": 21.289949417114258,
      "learning_rate": 1.544626593806922e-05,
      "loss": 1.4044,
      "step": 7500
    },
    {
      "epoch": 0.461445051608986,
      "grad_norm": 35.0536994934082,
      "learning_rate": 1.538554948391014e-05,
      "loss": 1.4728,
      "step": 7600
    },
    {
      "epoch": 0.46751669702489373,
      "grad_norm": 16.52252769470215,
      "learning_rate": 1.5324833029751063e-05,
      "loss": 1.3961,
      "step": 7700
    },
    {
      "epoch": 0.47358834244080145,
      "grad_norm": 33.12568283081055,
      "learning_rate": 1.526411657559199e-05,
      "loss": 1.3734,
      "step": 7800
    },
    {
      "epoch": 0.47965998785670916,
      "grad_norm": 25.07231903076172,
      "learning_rate": 1.5203400121432909e-05,
      "loss": 1.4311,
      "step": 7900
    },
    {
      "epoch": 0.48573163327261687,
      "grad_norm": 43.82021713256836,
      "learning_rate": 1.5142683667273831e-05,
      "loss": 1.4833,
      "step": 8000
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 16.152624130249023,
      "learning_rate": 1.5081967213114754e-05,
      "loss": 1.4357,
      "step": 8100
    },
    {
      "epoch": 0.4978749241044323,
      "grad_norm": 25.476961135864258,
      "learning_rate": 1.502125075895568e-05,
      "loss": 1.38,
      "step": 8200
    },
    {
      "epoch": 0.50394656952034,
      "grad_norm": 19.551677703857422,
      "learning_rate": 1.4960534304796602e-05,
      "loss": 1.3712,
      "step": 8300
    },
    {
      "epoch": 0.5100182149362478,
      "grad_norm": 20.78053855895996,
      "learning_rate": 1.4899817850637524e-05,
      "loss": 1.3842,
      "step": 8400
    },
    {
      "epoch": 0.5160898603521554,
      "grad_norm": 8.279583930969238,
      "learning_rate": 1.4839101396478446e-05,
      "loss": 1.4708,
      "step": 8500
    },
    {
      "epoch": 0.5221615057680632,
      "grad_norm": 21.942625045776367,
      "learning_rate": 1.477838494231937e-05,
      "loss": 1.4792,
      "step": 8600
    },
    {
      "epoch": 0.5282331511839709,
      "grad_norm": 13.500702857971191,
      "learning_rate": 1.4717668488160292e-05,
      "loss": 1.4262,
      "step": 8700
    },
    {
      "epoch": 0.5343047965998786,
      "grad_norm": 19.362268447875977,
      "learning_rate": 1.4656952034001215e-05,
      "loss": 1.5097,
      "step": 8800
    },
    {
      "epoch": 0.5403764420157863,
      "grad_norm": 16.131145477294922,
      "learning_rate": 1.4596235579842137e-05,
      "loss": 1.4532,
      "step": 8900
    },
    {
      "epoch": 0.546448087431694,
      "grad_norm": 17.23655891418457,
      "learning_rate": 1.4535519125683062e-05,
      "loss": 1.4328,
      "step": 9000
    },
    {
      "epoch": 0.5525197328476017,
      "grad_norm": 31.24182891845703,
      "learning_rate": 1.4474802671523985e-05,
      "loss": 1.4773,
      "step": 9100
    },
    {
      "epoch": 0.5585913782635095,
      "grad_norm": 19.929569244384766,
      "learning_rate": 1.4414086217364907e-05,
      "loss": 1.4415,
      "step": 9200
    },
    {
      "epoch": 0.5646630236794171,
      "grad_norm": 16.804668426513672,
      "learning_rate": 1.435336976320583e-05,
      "loss": 1.3762,
      "step": 9300
    },
    {
      "epoch": 0.5707346690953249,
      "grad_norm": 19.582782745361328,
      "learning_rate": 1.4292653309046753e-05,
      "loss": 1.4189,
      "step": 9400
    },
    {
      "epoch": 0.5768063145112325,
      "grad_norm": 15.05738639831543,
      "learning_rate": 1.4231936854887675e-05,
      "loss": 1.3335,
      "step": 9500
    },
    {
      "epoch": 0.5828779599271403,
      "grad_norm": 14.130531311035156,
      "learning_rate": 1.4171220400728598e-05,
      "loss": 1.4085,
      "step": 9600
    },
    {
      "epoch": 0.588949605343048,
      "grad_norm": 18.10894775390625,
      "learning_rate": 1.4110503946569523e-05,
      "loss": 1.4402,
      "step": 9700
    },
    {
      "epoch": 0.5950212507589556,
      "grad_norm": 18.544111251831055,
      "learning_rate": 1.4049787492410446e-05,
      "loss": 1.2679,
      "step": 9800
    },
    {
      "epoch": 0.6010928961748634,
      "grad_norm": 12.670514106750488,
      "learning_rate": 1.3989071038251368e-05,
      "loss": 1.3216,
      "step": 9900
    },
    {
      "epoch": 0.607164541590771,
      "grad_norm": 16.284801483154297,
      "learning_rate": 1.392835458409229e-05,
      "loss": 1.3396,
      "step": 10000
    },
    {
      "epoch": 0.6132361870066788,
      "grad_norm": 41.70616149902344,
      "learning_rate": 1.3867638129933214e-05,
      "loss": 1.38,
      "step": 10100
    },
    {
      "epoch": 0.6193078324225865,
      "grad_norm": 6.528571128845215,
      "learning_rate": 1.3806921675774136e-05,
      "loss": 1.426,
      "step": 10200
    },
    {
      "epoch": 0.6253794778384942,
      "grad_norm": 53.03622817993164,
      "learning_rate": 1.3746205221615059e-05,
      "loss": 1.3747,
      "step": 10300
    },
    {
      "epoch": 0.6314511232544019,
      "grad_norm": 9.3684720993042,
      "learning_rate": 1.368548876745598e-05,
      "loss": 1.3717,
      "step": 10400
    },
    {
      "epoch": 0.6375227686703097,
      "grad_norm": 21.23341178894043,
      "learning_rate": 1.3624772313296906e-05,
      "loss": 1.2517,
      "step": 10500
    },
    {
      "epoch": 0.6435944140862173,
      "grad_norm": 21.24834632873535,
      "learning_rate": 1.3564055859137829e-05,
      "loss": 1.3545,
      "step": 10600
    },
    {
      "epoch": 0.6496660595021251,
      "grad_norm": 12.156889915466309,
      "learning_rate": 1.3503339404978751e-05,
      "loss": 1.3879,
      "step": 10700
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 28.569732666015625,
      "learning_rate": 1.3442622950819673e-05,
      "loss": 1.3646,
      "step": 10800
    },
    {
      "epoch": 0.6618093503339405,
      "grad_norm": 18.82826805114746,
      "learning_rate": 1.3381906496660597e-05,
      "loss": 1.4716,
      "step": 10900
    },
    {
      "epoch": 0.6678809957498482,
      "grad_norm": 20.62760353088379,
      "learning_rate": 1.332119004250152e-05,
      "loss": 1.3442,
      "step": 11000
    },
    {
      "epoch": 0.6739526411657559,
      "grad_norm": 22.21361541748047,
      "learning_rate": 1.3260473588342442e-05,
      "loss": 1.371,
      "step": 11100
    },
    {
      "epoch": 0.6800242865816636,
      "grad_norm": 21.08370018005371,
      "learning_rate": 1.3199757134183364e-05,
      "loss": 1.3984,
      "step": 11200
    },
    {
      "epoch": 0.6860959319975714,
      "grad_norm": 34.676963806152344,
      "learning_rate": 1.3139040680024288e-05,
      "loss": 1.4646,
      "step": 11300
    },
    {
      "epoch": 0.692167577413479,
      "grad_norm": 11.107781410217285,
      "learning_rate": 1.307832422586521e-05,
      "loss": 1.3988,
      "step": 11400
    },
    {
      "epoch": 0.6982392228293868,
      "grad_norm": 27.688982009887695,
      "learning_rate": 1.3017607771706134e-05,
      "loss": 1.3226,
      "step": 11500
    },
    {
      "epoch": 0.7043108682452944,
      "grad_norm": 23.969987869262695,
      "learning_rate": 1.2956891317547056e-05,
      "loss": 1.3347,
      "step": 11600
    },
    {
      "epoch": 0.7103825136612022,
      "grad_norm": 23.36321258544922,
      "learning_rate": 1.289617486338798e-05,
      "loss": 1.4263,
      "step": 11700
    },
    {
      "epoch": 0.7164541590771099,
      "grad_norm": 14.796408653259277,
      "learning_rate": 1.2835458409228903e-05,
      "loss": 1.3012,
      "step": 11800
    },
    {
      "epoch": 0.7225258044930176,
      "grad_norm": 37.18631362915039,
      "learning_rate": 1.2774741955069825e-05,
      "loss": 1.3099,
      "step": 11900
    },
    {
      "epoch": 0.7285974499089253,
      "grad_norm": 13.765080451965332,
      "learning_rate": 1.2714025500910747e-05,
      "loss": 1.3394,
      "step": 12000
    },
    {
      "epoch": 0.734669095324833,
      "grad_norm": 64.35848999023438,
      "learning_rate": 1.2653309046751671e-05,
      "loss": 1.3431,
      "step": 12100
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 39.097923278808594,
      "learning_rate": 1.2592592592592593e-05,
      "loss": 1.3785,
      "step": 12200
    },
    {
      "epoch": 0.7468123861566485,
      "grad_norm": 31.854198455810547,
      "learning_rate": 1.2531876138433516e-05,
      "loss": 1.3521,
      "step": 12300
    },
    {
      "epoch": 0.7528840315725561,
      "grad_norm": 12.823427200317383,
      "learning_rate": 1.247115968427444e-05,
      "loss": 1.4084,
      "step": 12400
    },
    {
      "epoch": 0.7589556769884639,
      "grad_norm": 14.020084381103516,
      "learning_rate": 1.2410443230115363e-05,
      "loss": 1.3385,
      "step": 12500
    },
    {
      "epoch": 0.7650273224043715,
      "grad_norm": 20.53643798828125,
      "learning_rate": 1.2349726775956286e-05,
      "loss": 1.328,
      "step": 12600
    },
    {
      "epoch": 0.7710989678202793,
      "grad_norm": 24.135656356811523,
      "learning_rate": 1.2289010321797208e-05,
      "loss": 1.3866,
      "step": 12700
    },
    {
      "epoch": 0.777170613236187,
      "grad_norm": 27.215164184570312,
      "learning_rate": 1.222829386763813e-05,
      "loss": 1.3133,
      "step": 12800
    },
    {
      "epoch": 0.7832422586520947,
      "grad_norm": 32.85581970214844,
      "learning_rate": 1.2167577413479054e-05,
      "loss": 1.4234,
      "step": 12900
    },
    {
      "epoch": 0.7893139040680024,
      "grad_norm": 45.45546340942383,
      "learning_rate": 1.2106860959319976e-05,
      "loss": 1.3502,
      "step": 13000
    },
    {
      "epoch": 0.7953855494839102,
      "grad_norm": 3.866055965423584,
      "learning_rate": 1.2046144505160899e-05,
      "loss": 1.3274,
      "step": 13100
    },
    {
      "epoch": 0.8014571948998178,
      "grad_norm": 30.376605987548828,
      "learning_rate": 1.1985428051001821e-05,
      "loss": 1.283,
      "step": 13200
    },
    {
      "epoch": 0.8075288403157256,
      "grad_norm": 17.974870681762695,
      "learning_rate": 1.1924711596842747e-05,
      "loss": 1.3729,
      "step": 13300
    },
    {
      "epoch": 0.8136004857316332,
      "grad_norm": 17.81493377685547,
      "learning_rate": 1.1863995142683669e-05,
      "loss": 1.3555,
      "step": 13400
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 32.74492645263672,
      "learning_rate": 1.1803278688524591e-05,
      "loss": 1.3436,
      "step": 13500
    },
    {
      "epoch": 0.8257437765634487,
      "grad_norm": 22.810827255249023,
      "learning_rate": 1.1742562234365513e-05,
      "loss": 1.3605,
      "step": 13600
    },
    {
      "epoch": 0.8318154219793564,
      "grad_norm": 47.99007797241211,
      "learning_rate": 1.1681845780206437e-05,
      "loss": 1.2779,
      "step": 13700
    },
    {
      "epoch": 0.8378870673952641,
      "grad_norm": 20.353296279907227,
      "learning_rate": 1.162112932604736e-05,
      "loss": 1.4123,
      "step": 13800
    },
    {
      "epoch": 0.8439587128111719,
      "grad_norm": 17.272293090820312,
      "learning_rate": 1.1560412871888282e-05,
      "loss": 1.3472,
      "step": 13900
    },
    {
      "epoch": 0.8500303582270795,
      "grad_norm": 15.608070373535156,
      "learning_rate": 1.1499696417729204e-05,
      "loss": 1.3289,
      "step": 14000
    },
    {
      "epoch": 0.8561020036429873,
      "grad_norm": 16.816646575927734,
      "learning_rate": 1.143897996357013e-05,
      "loss": 1.3302,
      "step": 14100
    },
    {
      "epoch": 0.8621736490588949,
      "grad_norm": 16.382713317871094,
      "learning_rate": 1.1378263509411052e-05,
      "loss": 1.3309,
      "step": 14200
    },
    {
      "epoch": 0.8682452944748027,
      "grad_norm": 11.935954093933105,
      "learning_rate": 1.1317547055251974e-05,
      "loss": 1.3653,
      "step": 14300
    },
    {
      "epoch": 0.8743169398907104,
      "grad_norm": 20.81771469116211,
      "learning_rate": 1.1256830601092897e-05,
      "loss": 1.3699,
      "step": 14400
    },
    {
      "epoch": 0.8803885853066181,
      "grad_norm": 34.74507141113281,
      "learning_rate": 1.119611414693382e-05,
      "loss": 1.3907,
      "step": 14500
    },
    {
      "epoch": 0.8864602307225258,
      "grad_norm": 20.780372619628906,
      "learning_rate": 1.1135397692774743e-05,
      "loss": 1.3322,
      "step": 14600
    },
    {
      "epoch": 0.8925318761384335,
      "grad_norm": 20.61725425720215,
      "learning_rate": 1.1074681238615665e-05,
      "loss": 1.3806,
      "step": 14700
    },
    {
      "epoch": 0.8986035215543412,
      "grad_norm": 29.264556884765625,
      "learning_rate": 1.1013964784456587e-05,
      "loss": 1.3796,
      "step": 14800
    },
    {
      "epoch": 0.904675166970249,
      "grad_norm": 25.27979850769043,
      "learning_rate": 1.0953248330297513e-05,
      "loss": 1.343,
      "step": 14900
    },
    {
      "epoch": 0.9107468123861566,
      "grad_norm": 15.622150421142578,
      "learning_rate": 1.0892531876138435e-05,
      "loss": 1.3413,
      "step": 15000
    },
    {
      "epoch": 0.9168184578020644,
      "grad_norm": 17.76564598083496,
      "learning_rate": 1.0831815421979357e-05,
      "loss": 1.3076,
      "step": 15100
    },
    {
      "epoch": 0.922890103217972,
      "grad_norm": 10.593975067138672,
      "learning_rate": 1.077109896782028e-05,
      "loss": 1.2804,
      "step": 15200
    },
    {
      "epoch": 0.9289617486338798,
      "grad_norm": 34.68490982055664,
      "learning_rate": 1.0710382513661204e-05,
      "loss": 1.3346,
      "step": 15300
    },
    {
      "epoch": 0.9350333940497875,
      "grad_norm": 37.239723205566406,
      "learning_rate": 1.0649666059502126e-05,
      "loss": 1.3117,
      "step": 15400
    },
    {
      "epoch": 0.9411050394656952,
      "grad_norm": 20.66153335571289,
      "learning_rate": 1.0588949605343048e-05,
      "loss": 1.3195,
      "step": 15500
    },
    {
      "epoch": 0.9471766848816029,
      "grad_norm": 23.84140968322754,
      "learning_rate": 1.052823315118397e-05,
      "loss": 1.2573,
      "step": 15600
    },
    {
      "epoch": 0.9532483302975107,
      "grad_norm": 22.295421600341797,
      "learning_rate": 1.0467516697024896e-05,
      "loss": 1.3242,
      "step": 15700
    },
    {
      "epoch": 0.9593199757134183,
      "grad_norm": 52.20244598388672,
      "learning_rate": 1.0406800242865818e-05,
      "loss": 1.405,
      "step": 15800
    },
    {
      "epoch": 0.9653916211293261,
      "grad_norm": 53.490882873535156,
      "learning_rate": 1.034608378870674e-05,
      "loss": 1.3206,
      "step": 15900
    },
    {
      "epoch": 0.9714632665452337,
      "grad_norm": 33.75416564941406,
      "learning_rate": 1.0285367334547663e-05,
      "loss": 1.4452,
      "step": 16000
    },
    {
      "epoch": 0.9775349119611415,
      "grad_norm": 22.605735778808594,
      "learning_rate": 1.0224650880388587e-05,
      "loss": 1.3662,
      "step": 16100
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 18.9329776763916,
      "learning_rate": 1.0163934426229509e-05,
      "loss": 1.2806,
      "step": 16200
    },
    {
      "epoch": 0.9896782027929569,
      "grad_norm": 16.06412124633789,
      "learning_rate": 1.0103217972070431e-05,
      "loss": 1.2662,
      "step": 16300
    },
    {
      "epoch": 0.9957498482088646,
      "grad_norm": 17.541458129882812,
      "learning_rate": 1.0042501517911354e-05,
      "loss": 1.2533,
      "step": 16400
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.4866690635681152,
      "eval_runtime": 36.9733,
      "eval_samples_per_second": 328.183,
      "eval_steps_per_second": 41.03,
      "step": 16470
    },
    {
      "epoch": 1.0018214936247722,
      "grad_norm": 15.331103324890137,
      "learning_rate": 9.981785063752277e-06,
      "loss": 1.2525,
      "step": 16500
    },
    {
      "epoch": 1.00789313904068,
      "grad_norm": 16.921499252319336,
      "learning_rate": 9.921068609593201e-06,
      "loss": 1.2417,
      "step": 16600
    },
    {
      "epoch": 1.0139647844565878,
      "grad_norm": 36.15134048461914,
      "learning_rate": 9.860352155434124e-06,
      "loss": 1.0864,
      "step": 16700
    },
    {
      "epoch": 1.0200364298724955,
      "grad_norm": 85.94098663330078,
      "learning_rate": 9.799635701275046e-06,
      "loss": 1.1601,
      "step": 16800
    },
    {
      "epoch": 1.026108075288403,
      "grad_norm": 23.36921501159668,
      "learning_rate": 9.738919247115968e-06,
      "loss": 1.0612,
      "step": 16900
    },
    {
      "epoch": 1.0321797207043109,
      "grad_norm": 19.953031539916992,
      "learning_rate": 9.678202792956892e-06,
      "loss": 1.203,
      "step": 17000
    },
    {
      "epoch": 1.0382513661202186,
      "grad_norm": 9.140584945678711,
      "learning_rate": 9.617486338797814e-06,
      "loss": 1.1685,
      "step": 17100
    },
    {
      "epoch": 1.0443230115361264,
      "grad_norm": 12.398907661437988,
      "learning_rate": 9.556769884638738e-06,
      "loss": 1.1961,
      "step": 17200
    },
    {
      "epoch": 1.050394656952034,
      "grad_norm": 10.18558120727539,
      "learning_rate": 9.49605343047966e-06,
      "loss": 1.0898,
      "step": 17300
    },
    {
      "epoch": 1.0564663023679417,
      "grad_norm": 21.340030670166016,
      "learning_rate": 9.435336976320585e-06,
      "loss": 1.1703,
      "step": 17400
    },
    {
      "epoch": 1.0625379477838495,
      "grad_norm": 54.32823181152344,
      "learning_rate": 9.374620522161507e-06,
      "loss": 1.2253,
      "step": 17500
    },
    {
      "epoch": 1.0686095931997572,
      "grad_norm": 4.403931617736816,
      "learning_rate": 9.313904068002429e-06,
      "loss": 1.165,
      "step": 17600
    },
    {
      "epoch": 1.0746812386156648,
      "grad_norm": 28.890701293945312,
      "learning_rate": 9.253187613843351e-06,
      "loss": 1.1414,
      "step": 17700
    },
    {
      "epoch": 1.0807528840315725,
      "grad_norm": 25.44788932800293,
      "learning_rate": 9.192471159684275e-06,
      "loss": 1.1769,
      "step": 17800
    },
    {
      "epoch": 1.0868245294474803,
      "grad_norm": 30.107778549194336,
      "learning_rate": 9.131754705525198e-06,
      "loss": 1.1319,
      "step": 17900
    },
    {
      "epoch": 1.092896174863388,
      "grad_norm": 42.1945686340332,
      "learning_rate": 9.071038251366122e-06,
      "loss": 1.1438,
      "step": 18000
    },
    {
      "epoch": 1.0989678202792956,
      "grad_norm": 28.338674545288086,
      "learning_rate": 9.010321797207044e-06,
      "loss": 1.239,
      "step": 18100
    },
    {
      "epoch": 1.1050394656952034,
      "grad_norm": 40.187992095947266,
      "learning_rate": 8.949605343047966e-06,
      "loss": 1.1527,
      "step": 18200
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 48.02280807495117,
      "learning_rate": 8.888888888888888e-06,
      "loss": 1.2385,
      "step": 18300
    },
    {
      "epoch": 1.117182756527019,
      "grad_norm": 23.404489517211914,
      "learning_rate": 8.828172434729812e-06,
      "loss": 1.1486,
      "step": 18400
    },
    {
      "epoch": 1.1232544019429265,
      "grad_norm": 52.3139762878418,
      "learning_rate": 8.767455980570735e-06,
      "loss": 1.1638,
      "step": 18500
    },
    {
      "epoch": 1.1293260473588342,
      "grad_norm": 11.442070960998535,
      "learning_rate": 8.706739526411658e-06,
      "loss": 1.2714,
      "step": 18600
    },
    {
      "epoch": 1.135397692774742,
      "grad_norm": 36.97091293334961,
      "learning_rate": 8.64602307225258e-06,
      "loss": 1.2486,
      "step": 18700
    },
    {
      "epoch": 1.1414693381906496,
      "grad_norm": 29.354656219482422,
      "learning_rate": 8.585306618093505e-06,
      "loss": 1.2552,
      "step": 18800
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 27.07468605041504,
      "learning_rate": 8.524590163934427e-06,
      "loss": 1.2162,
      "step": 18900
    },
    {
      "epoch": 1.153612629022465,
      "grad_norm": 17.54111099243164,
      "learning_rate": 8.46387370977535e-06,
      "loss": 1.1992,
      "step": 19000
    },
    {
      "epoch": 1.1596842744383729,
      "grad_norm": 19.110321044921875,
      "learning_rate": 8.403157255616271e-06,
      "loss": 1.2436,
      "step": 19100
    },
    {
      "epoch": 1.1657559198542806,
      "grad_norm": 27.048500061035156,
      "learning_rate": 8.342440801457195e-06,
      "loss": 1.1901,
      "step": 19200
    },
    {
      "epoch": 1.1718275652701882,
      "grad_norm": 9.95876407623291,
      "learning_rate": 8.281724347298118e-06,
      "loss": 1.1554,
      "step": 19300
    },
    {
      "epoch": 1.177899210686096,
      "grad_norm": 29.914997100830078,
      "learning_rate": 8.221007893139042e-06,
      "loss": 1.1927,
      "step": 19400
    },
    {
      "epoch": 1.1839708561020037,
      "grad_norm": 25.159446716308594,
      "learning_rate": 8.160291438979966e-06,
      "loss": 1.1493,
      "step": 19500
    },
    {
      "epoch": 1.1900425015179112,
      "grad_norm": 35.9827766418457,
      "learning_rate": 8.099574984820888e-06,
      "loss": 1.1669,
      "step": 19600
    },
    {
      "epoch": 1.196114146933819,
      "grad_norm": 35.96525573730469,
      "learning_rate": 8.03885853066181e-06,
      "loss": 1.189,
      "step": 19700
    },
    {
      "epoch": 1.2021857923497268,
      "grad_norm": 17.303476333618164,
      "learning_rate": 7.978142076502732e-06,
      "loss": 1.1779,
      "step": 19800
    },
    {
      "epoch": 1.2082574377656345,
      "grad_norm": 13.188318252563477,
      "learning_rate": 7.917425622343656e-06,
      "loss": 1.1689,
      "step": 19900
    },
    {
      "epoch": 1.2143290831815423,
      "grad_norm": 14.102816581726074,
      "learning_rate": 7.856709168184579e-06,
      "loss": 1.1553,
      "step": 20000
    },
    {
      "epoch": 1.2204007285974499,
      "grad_norm": 14.807283401489258,
      "learning_rate": 7.795992714025502e-06,
      "loss": 1.2032,
      "step": 20100
    },
    {
      "epoch": 1.2264723740133576,
      "grad_norm": 27.225461959838867,
      "learning_rate": 7.735276259866425e-06,
      "loss": 1.1454,
      "step": 20200
    },
    {
      "epoch": 1.2325440194292654,
      "grad_norm": 25.139982223510742,
      "learning_rate": 7.674559805707347e-06,
      "loss": 1.1639,
      "step": 20300
    },
    {
      "epoch": 1.238615664845173,
      "grad_norm": 21.718393325805664,
      "learning_rate": 7.61384335154827e-06,
      "loss": 1.1374,
      "step": 20400
    },
    {
      "epoch": 1.2446873102610807,
      "grad_norm": 35.50840377807617,
      "learning_rate": 7.553126897389193e-06,
      "loss": 1.1958,
      "step": 20500
    },
    {
      "epoch": 1.2507589556769885,
      "grad_norm": 12.321331024169922,
      "learning_rate": 7.492410443230116e-06,
      "loss": 1.2124,
      "step": 20600
    },
    {
      "epoch": 1.2568306010928962,
      "grad_norm": 17.34304428100586,
      "learning_rate": 7.4316939890710394e-06,
      "loss": 1.1684,
      "step": 20700
    },
    {
      "epoch": 1.262902246508804,
      "grad_norm": 39.05267333984375,
      "learning_rate": 7.370977534911962e-06,
      "loss": 1.2032,
      "step": 20800
    },
    {
      "epoch": 1.2689738919247116,
      "grad_norm": 26.20633316040039,
      "learning_rate": 7.310261080752885e-06,
      "loss": 1.1507,
      "step": 20900
    },
    {
      "epoch": 1.2750455373406193,
      "grad_norm": 13.120933532714844,
      "learning_rate": 7.249544626593807e-06,
      "loss": 1.1519,
      "step": 21000
    },
    {
      "epoch": 1.281117182756527,
      "grad_norm": 34.72346878051758,
      "learning_rate": 7.188828172434731e-06,
      "loss": 1.217,
      "step": 21100
    },
    {
      "epoch": 1.2871888281724346,
      "grad_norm": 18.28764533996582,
      "learning_rate": 7.128111718275653e-06,
      "loss": 1.175,
      "step": 21200
    },
    {
      "epoch": 1.2932604735883424,
      "grad_norm": 25.096670150756836,
      "learning_rate": 7.067395264116576e-06,
      "loss": 1.089,
      "step": 21300
    },
    {
      "epoch": 1.2993321190042502,
      "grad_norm": 15.558796882629395,
      "learning_rate": 7.006678809957499e-06,
      "loss": 1.206,
      "step": 21400
    },
    {
      "epoch": 1.305403764420158,
      "grad_norm": 23.66599464416504,
      "learning_rate": 6.9459623557984226e-06,
      "loss": 1.1729,
      "step": 21500
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 19.073850631713867,
      "learning_rate": 6.885245901639345e-06,
      "loss": 1.1465,
      "step": 21600
    },
    {
      "epoch": 1.3175470552519732,
      "grad_norm": 40.26721954345703,
      "learning_rate": 6.824529447480268e-06,
      "loss": 1.1205,
      "step": 21700
    },
    {
      "epoch": 1.323618700667881,
      "grad_norm": 12.716398239135742,
      "learning_rate": 6.76381299332119e-06,
      "loss": 1.171,
      "step": 21800
    },
    {
      "epoch": 1.3296903460837888,
      "grad_norm": 25.78591537475586,
      "learning_rate": 6.703096539162114e-06,
      "loss": 1.1248,
      "step": 21900
    },
    {
      "epoch": 1.3357619914996963,
      "grad_norm": 13.355960845947266,
      "learning_rate": 6.642380085003036e-06,
      "loss": 1.2211,
      "step": 22000
    },
    {
      "epoch": 1.341833636915604,
      "grad_norm": 11.839824676513672,
      "learning_rate": 6.5816636308439595e-06,
      "loss": 1.1616,
      "step": 22100
    },
    {
      "epoch": 1.3479052823315119,
      "grad_norm": 18.685449600219727,
      "learning_rate": 6.520947176684882e-06,
      "loss": 1.2049,
      "step": 22200
    },
    {
      "epoch": 1.3539769277474196,
      "grad_norm": 42.99837112426758,
      "learning_rate": 6.460230722525806e-06,
      "loss": 1.2292,
      "step": 22300
    },
    {
      "epoch": 1.3600485731633274,
      "grad_norm": 28.95188331604004,
      "learning_rate": 6.399514268366728e-06,
      "loss": 1.1828,
      "step": 22400
    },
    {
      "epoch": 1.366120218579235,
      "grad_norm": 11.238668441772461,
      "learning_rate": 6.338797814207651e-06,
      "loss": 1.1236,
      "step": 22500
    },
    {
      "epoch": 1.3721918639951427,
      "grad_norm": 12.506240844726562,
      "learning_rate": 6.278081360048573e-06,
      "loss": 1.136,
      "step": 22600
    },
    {
      "epoch": 1.3782635094110505,
      "grad_norm": 22.741622924804688,
      "learning_rate": 6.217364905889497e-06,
      "loss": 1.1457,
      "step": 22700
    },
    {
      "epoch": 1.384335154826958,
      "grad_norm": 18.419071197509766,
      "learning_rate": 6.1566484517304195e-06,
      "loss": 1.1947,
      "step": 22800
    },
    {
      "epoch": 1.3904068002428658,
      "grad_norm": 10.962857246398926,
      "learning_rate": 6.095931997571343e-06,
      "loss": 1.2004,
      "step": 22900
    },
    {
      "epoch": 1.3964784456587735,
      "grad_norm": 15.969732284545898,
      "learning_rate": 6.035215543412265e-06,
      "loss": 1.1349,
      "step": 23000
    },
    {
      "epoch": 1.4025500910746813,
      "grad_norm": 27.699708938598633,
      "learning_rate": 5.974499089253189e-06,
      "loss": 1.162,
      "step": 23100
    },
    {
      "epoch": 1.408621736490589,
      "grad_norm": 30.410221099853516,
      "learning_rate": 5.913782635094111e-06,
      "loss": 1.2004,
      "step": 23200
    },
    {
      "epoch": 1.4146933819064966,
      "grad_norm": 29.531679153442383,
      "learning_rate": 5.853066180935034e-06,
      "loss": 1.1779,
      "step": 23300
    },
    {
      "epoch": 1.4207650273224044,
      "grad_norm": 31.56997299194336,
      "learning_rate": 5.7923497267759565e-06,
      "loss": 1.1163,
      "step": 23400
    },
    {
      "epoch": 1.4268366727383122,
      "grad_norm": 30.64568519592285,
      "learning_rate": 5.73163327261688e-06,
      "loss": 1.2199,
      "step": 23500
    },
    {
      "epoch": 1.4329083181542197,
      "grad_norm": 25.75310516357422,
      "learning_rate": 5.670916818457803e-06,
      "loss": 1.2049,
      "step": 23600
    },
    {
      "epoch": 1.4389799635701275,
      "grad_norm": 14.439123153686523,
      "learning_rate": 5.610200364298726e-06,
      "loss": 1.1989,
      "step": 23700
    },
    {
      "epoch": 1.4450516089860352,
      "grad_norm": 47.420684814453125,
      "learning_rate": 5.549483910139648e-06,
      "loss": 1.1731,
      "step": 23800
    },
    {
      "epoch": 1.451123254401943,
      "grad_norm": 8.895816802978516,
      "learning_rate": 5.488767455980571e-06,
      "loss": 1.1951,
      "step": 23900
    },
    {
      "epoch": 1.4571948998178508,
      "grad_norm": 22.437761306762695,
      "learning_rate": 5.428051001821493e-06,
      "loss": 1.1664,
      "step": 24000
    },
    {
      "epoch": 1.4632665452337583,
      "grad_norm": 26.92656135559082,
      "learning_rate": 5.367334547662417e-06,
      "loss": 1.1905,
      "step": 24100
    },
    {
      "epoch": 1.469338190649666,
      "grad_norm": 9.634474754333496,
      "learning_rate": 5.30661809350334e-06,
      "loss": 1.1537,
      "step": 24200
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 8.832694053649902,
      "learning_rate": 5.245901639344263e-06,
      "loss": 1.1437,
      "step": 24300
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 32.3863410949707,
      "learning_rate": 5.185185185185185e-06,
      "loss": 1.0964,
      "step": 24400
    },
    {
      "epoch": 1.4875531268973892,
      "grad_norm": 16.57790184020996,
      "learning_rate": 5.124468731026109e-06,
      "loss": 1.1793,
      "step": 24500
    },
    {
      "epoch": 1.493624772313297,
      "grad_norm": 16.522085189819336,
      "learning_rate": 5.063752276867031e-06,
      "loss": 1.1358,
      "step": 24600
    },
    {
      "epoch": 1.4996964177292047,
      "grad_norm": 19.654964447021484,
      "learning_rate": 5.003035822707954e-06,
      "loss": 1.2085,
      "step": 24700
    },
    {
      "epoch": 1.5057680631451125,
      "grad_norm": 27.35599136352539,
      "learning_rate": 4.942319368548877e-06,
      "loss": 1.1947,
      "step": 24800
    },
    {
      "epoch": 1.51183970856102,
      "grad_norm": 20.63072967529297,
      "learning_rate": 4.8816029143898005e-06,
      "loss": 1.0863,
      "step": 24900
    },
    {
      "epoch": 1.5179113539769278,
      "grad_norm": 15.65556812286377,
      "learning_rate": 4.820886460230723e-06,
      "loss": 1.1764,
      "step": 25000
    },
    {
      "epoch": 1.5239829993928353,
      "grad_norm": 19.576904296875,
      "learning_rate": 4.760170006071646e-06,
      "loss": 1.082,
      "step": 25100
    },
    {
      "epoch": 1.530054644808743,
      "grad_norm": 28.960617065429688,
      "learning_rate": 4.699453551912569e-06,
      "loss": 1.1374,
      "step": 25200
    },
    {
      "epoch": 1.5361262902246509,
      "grad_norm": 7.6554436683654785,
      "learning_rate": 4.638737097753492e-06,
      "loss": 1.0748,
      "step": 25300
    },
    {
      "epoch": 1.5421979356405586,
      "grad_norm": 24.697799682617188,
      "learning_rate": 4.578020643594414e-06,
      "loss": 1.2262,
      "step": 25400
    },
    {
      "epoch": 1.5482695810564664,
      "grad_norm": 8.331122398376465,
      "learning_rate": 4.5173041894353374e-06,
      "loss": 1.1249,
      "step": 25500
    },
    {
      "epoch": 1.5543412264723742,
      "grad_norm": 24.84864044189453,
      "learning_rate": 4.4565877352762605e-06,
      "loss": 1.1994,
      "step": 25600
    },
    {
      "epoch": 1.5604128718882817,
      "grad_norm": 33.088623046875,
      "learning_rate": 4.395871281117184e-06,
      "loss": 1.0798,
      "step": 25700
    },
    {
      "epoch": 1.5664845173041895,
      "grad_norm": 15.734925270080566,
      "learning_rate": 4.335154826958106e-06,
      "loss": 1.1826,
      "step": 25800
    },
    {
      "epoch": 1.572556162720097,
      "grad_norm": 14.293774604797363,
      "learning_rate": 4.274438372799029e-06,
      "loss": 1.0847,
      "step": 25900
    },
    {
      "epoch": 1.5786278081360048,
      "grad_norm": 6.5402326583862305,
      "learning_rate": 4.213721918639952e-06,
      "loss": 1.1663,
      "step": 26000
    },
    {
      "epoch": 1.5846994535519126,
      "grad_norm": 23.224666595458984,
      "learning_rate": 4.153005464480875e-06,
      "loss": 1.1882,
      "step": 26100
    },
    {
      "epoch": 1.5907710989678203,
      "grad_norm": 19.53958511352539,
      "learning_rate": 4.0922890103217975e-06,
      "loss": 1.1818,
      "step": 26200
    },
    {
      "epoch": 1.596842744383728,
      "grad_norm": 32.04662322998047,
      "learning_rate": 4.031572556162721e-06,
      "loss": 1.1313,
      "step": 26300
    },
    {
      "epoch": 1.6029143897996359,
      "grad_norm": 32.303436279296875,
      "learning_rate": 3.970856102003644e-06,
      "loss": 1.1344,
      "step": 26400
    },
    {
      "epoch": 1.6089860352155434,
      "grad_norm": 19.490495681762695,
      "learning_rate": 3.910139647844566e-06,
      "loss": 1.2161,
      "step": 26500
    },
    {
      "epoch": 1.6150576806314512,
      "grad_norm": 15.678266525268555,
      "learning_rate": 3.849423193685489e-06,
      "loss": 1.243,
      "step": 26600
    },
    {
      "epoch": 1.6211293260473587,
      "grad_norm": 17.44416046142578,
      "learning_rate": 3.788706739526412e-06,
      "loss": 1.1186,
      "step": 26700
    },
    {
      "epoch": 1.6272009714632665,
      "grad_norm": 16.39103126525879,
      "learning_rate": 3.727990285367335e-06,
      "loss": 1.1197,
      "step": 26800
    },
    {
      "epoch": 1.6332726168791742,
      "grad_norm": 37.01487731933594,
      "learning_rate": 3.667273831208258e-06,
      "loss": 1.1218,
      "step": 26900
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 21.652498245239258,
      "learning_rate": 3.6065573770491806e-06,
      "loss": 1.128,
      "step": 27000
    },
    {
      "epoch": 1.6454159077109898,
      "grad_norm": 25.490657806396484,
      "learning_rate": 3.5458409228901037e-06,
      "loss": 1.1255,
      "step": 27100
    },
    {
      "epoch": 1.6514875531268975,
      "grad_norm": 28.911216735839844,
      "learning_rate": 3.4851244687310264e-06,
      "loss": 1.102,
      "step": 27200
    },
    {
      "epoch": 1.657559198542805,
      "grad_norm": 25.53538703918457,
      "learning_rate": 3.4244080145719495e-06,
      "loss": 1.13,
      "step": 27300
    },
    {
      "epoch": 1.6636308439587129,
      "grad_norm": 5.775641441345215,
      "learning_rate": 3.363691560412872e-06,
      "loss": 1.0913,
      "step": 27400
    },
    {
      "epoch": 1.6697024893746204,
      "grad_norm": 25.488706588745117,
      "learning_rate": 3.3029751062537953e-06,
      "loss": 1.1847,
      "step": 27500
    },
    {
      "epoch": 1.6757741347905282,
      "grad_norm": 13.558696746826172,
      "learning_rate": 3.242258652094718e-06,
      "loss": 1.1953,
      "step": 27600
    },
    {
      "epoch": 1.681845780206436,
      "grad_norm": 8.52089786529541,
      "learning_rate": 3.181542197935641e-06,
      "loss": 1.0803,
      "step": 27700
    },
    {
      "epoch": 1.6879174256223437,
      "grad_norm": 12.441696166992188,
      "learning_rate": 3.1208257437765638e-06,
      "loss": 1.1869,
      "step": 27800
    },
    {
      "epoch": 1.6939890710382515,
      "grad_norm": 24.12432861328125,
      "learning_rate": 3.0601092896174864e-06,
      "loss": 1.1598,
      "step": 27900
    },
    {
      "epoch": 1.7000607164541592,
      "grad_norm": 9.515520095825195,
      "learning_rate": 2.9993928354584096e-06,
      "loss": 1.1324,
      "step": 28000
    },
    {
      "epoch": 1.7061323618700668,
      "grad_norm": 15.148786544799805,
      "learning_rate": 2.9386763812993322e-06,
      "loss": 1.1569,
      "step": 28100
    },
    {
      "epoch": 1.7122040072859745,
      "grad_norm": 23.272109985351562,
      "learning_rate": 2.8779599271402553e-06,
      "loss": 1.1191,
      "step": 28200
    },
    {
      "epoch": 1.718275652701882,
      "grad_norm": 37.66267395019531,
      "learning_rate": 2.817243472981178e-06,
      "loss": 1.1476,
      "step": 28300
    },
    {
      "epoch": 1.7243472981177899,
      "grad_norm": 25.26726531982422,
      "learning_rate": 2.756527018822101e-06,
      "loss": 1.173,
      "step": 28400
    },
    {
      "epoch": 1.7304189435336976,
      "grad_norm": 19.21967887878418,
      "learning_rate": 2.695810564663024e-06,
      "loss": 1.1216,
      "step": 28500
    },
    {
      "epoch": 1.7364905889496054,
      "grad_norm": 7.985748767852783,
      "learning_rate": 2.635094110503947e-06,
      "loss": 1.1602,
      "step": 28600
    },
    {
      "epoch": 1.7425622343655132,
      "grad_norm": 7.981356143951416,
      "learning_rate": 2.5743776563448696e-06,
      "loss": 1.1146,
      "step": 28700
    },
    {
      "epoch": 1.748633879781421,
      "grad_norm": 26.993541717529297,
      "learning_rate": 2.5136612021857927e-06,
      "loss": 1.1225,
      "step": 28800
    },
    {
      "epoch": 1.7547055251973285,
      "grad_norm": 15.547520637512207,
      "learning_rate": 2.4529447480267154e-06,
      "loss": 1.1927,
      "step": 28900
    },
    {
      "epoch": 1.7607771706132362,
      "grad_norm": 21.779176712036133,
      "learning_rate": 2.3922282938676385e-06,
      "loss": 1.1606,
      "step": 29000
    },
    {
      "epoch": 1.7668488160291438,
      "grad_norm": 11.765080451965332,
      "learning_rate": 2.331511839708561e-06,
      "loss": 1.175,
      "step": 29100
    },
    {
      "epoch": 1.7729204614450516,
      "grad_norm": 17.929391860961914,
      "learning_rate": 2.2707953855494843e-06,
      "loss": 1.0719,
      "step": 29200
    },
    {
      "epoch": 1.7789921068609593,
      "grad_norm": 31.57929801940918,
      "learning_rate": 2.210078931390407e-06,
      "loss": 1.0747,
      "step": 29300
    },
    {
      "epoch": 1.785063752276867,
      "grad_norm": 25.117645263671875,
      "learning_rate": 2.14936247723133e-06,
      "loss": 1.113,
      "step": 29400
    },
    {
      "epoch": 1.7911353976927749,
      "grad_norm": 35.238258361816406,
      "learning_rate": 2.0886460230722527e-06,
      "loss": 1.0957,
      "step": 29500
    },
    {
      "epoch": 1.7972070431086824,
      "grad_norm": 18.14760971069336,
      "learning_rate": 2.0279295689131754e-06,
      "loss": 1.2154,
      "step": 29600
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 17.878664016723633,
      "learning_rate": 1.9672131147540985e-06,
      "loss": 1.1548,
      "step": 29700
    },
    {
      "epoch": 1.8093503339404977,
      "grad_norm": 24.92666244506836,
      "learning_rate": 1.9064966605950214e-06,
      "loss": 1.0923,
      "step": 29800
    },
    {
      "epoch": 1.8154219793564055,
      "grad_norm": 15.808605194091797,
      "learning_rate": 1.8457802064359443e-06,
      "loss": 1.1905,
      "step": 29900
    },
    {
      "epoch": 1.8214936247723132,
      "grad_norm": 80.74221801757812,
      "learning_rate": 1.7850637522768672e-06,
      "loss": 1.0979,
      "step": 30000
    },
    {
      "epoch": 1.827565270188221,
      "grad_norm": 22.75328826904297,
      "learning_rate": 1.72434729811779e-06,
      "loss": 1.1766,
      "step": 30100
    },
    {
      "epoch": 1.8336369156041288,
      "grad_norm": 31.819162368774414,
      "learning_rate": 1.663630843958713e-06,
      "loss": 1.0564,
      "step": 30200
    },
    {
      "epoch": 1.8397085610200365,
      "grad_norm": 13.536312103271484,
      "learning_rate": 1.6029143897996357e-06,
      "loss": 1.1796,
      "step": 30300
    },
    {
      "epoch": 1.845780206435944,
      "grad_norm": 23.02435302734375,
      "learning_rate": 1.5421979356405586e-06,
      "loss": 1.1192,
      "step": 30400
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 32.57844543457031,
      "learning_rate": 1.4814814814814815e-06,
      "loss": 1.1755,
      "step": 30500
    },
    {
      "epoch": 1.8579234972677594,
      "grad_norm": 26.359638214111328,
      "learning_rate": 1.4207650273224043e-06,
      "loss": 1.1411,
      "step": 30600
    },
    {
      "epoch": 1.8639951426836672,
      "grad_norm": 16.102270126342773,
      "learning_rate": 1.3600485731633272e-06,
      "loss": 1.118,
      "step": 30700
    },
    {
      "epoch": 1.870066788099575,
      "grad_norm": 18.758726119995117,
      "learning_rate": 1.2993321190042501e-06,
      "loss": 1.0555,
      "step": 30800
    },
    {
      "epoch": 1.8761384335154827,
      "grad_norm": 19.71187973022461,
      "learning_rate": 1.2386156648451732e-06,
      "loss": 1.1369,
      "step": 30900
    },
    {
      "epoch": 1.8822100789313905,
      "grad_norm": 8.116143226623535,
      "learning_rate": 1.1778992106860961e-06,
      "loss": 1.1251,
      "step": 31000
    },
    {
      "epoch": 1.8882817243472982,
      "grad_norm": 9.382420539855957,
      "learning_rate": 1.117182756527019e-06,
      "loss": 1.1417,
      "step": 31100
    },
    {
      "epoch": 1.8943533697632058,
      "grad_norm": 21.009904861450195,
      "learning_rate": 1.056466302367942e-06,
      "loss": 1.1999,
      "step": 31200
    },
    {
      "epoch": 1.9004250151791136,
      "grad_norm": 22.63650894165039,
      "learning_rate": 9.957498482088646e-07,
      "loss": 1.0727,
      "step": 31300
    },
    {
      "epoch": 1.906496660595021,
      "grad_norm": 21.110363006591797,
      "learning_rate": 9.350333940497876e-07,
      "loss": 1.0933,
      "step": 31400
    },
    {
      "epoch": 1.9125683060109289,
      "grad_norm": 23.77118492126465,
      "learning_rate": 8.743169398907105e-07,
      "loss": 1.148,
      "step": 31500
    },
    {
      "epoch": 1.9186399514268366,
      "grad_norm": 27.808109283447266,
      "learning_rate": 8.136004857316334e-07,
      "loss": 1.1013,
      "step": 31600
    },
    {
      "epoch": 1.9247115968427444,
      "grad_norm": 24.434022903442383,
      "learning_rate": 7.528840315725562e-07,
      "loss": 1.1163,
      "step": 31700
    },
    {
      "epoch": 1.9307832422586522,
      "grad_norm": 25.107990264892578,
      "learning_rate": 6.921675774134791e-07,
      "loss": 1.2204,
      "step": 31800
    },
    {
      "epoch": 1.93685488767456,
      "grad_norm": 53.62290573120117,
      "learning_rate": 6.31451123254402e-07,
      "loss": 1.1073,
      "step": 31900
    },
    {
      "epoch": 1.9429265330904675,
      "grad_norm": 25.385583877563477,
      "learning_rate": 5.707346690953248e-07,
      "loss": 1.046,
      "step": 32000
    },
    {
      "epoch": 1.9489981785063752,
      "grad_norm": 23.205829620361328,
      "learning_rate": 5.100182149362478e-07,
      "loss": 1.1321,
      "step": 32100
    },
    {
      "epoch": 1.9550698239222828,
      "grad_norm": 20.795141220092773,
      "learning_rate": 4.493017607771707e-07,
      "loss": 1.0834,
      "step": 32200
    },
    {
      "epoch": 1.9611414693381906,
      "grad_norm": 21.824670791625977,
      "learning_rate": 3.885853066180936e-07,
      "loss": 1.1075,
      "step": 32300
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 50.856075286865234,
      "learning_rate": 3.278688524590164e-07,
      "loss": 1.2064,
      "step": 32400
    },
    {
      "epoch": 1.973284760170006,
      "grad_norm": 30.700576782226562,
      "learning_rate": 2.671523982999393e-07,
      "loss": 1.1448,
      "step": 32500
    },
    {
      "epoch": 1.9793564055859139,
      "grad_norm": 9.6697998046875,
      "learning_rate": 2.064359441408622e-07,
      "loss": 1.0304,
      "step": 32600
    },
    {
      "epoch": 1.9854280510018216,
      "grad_norm": 22.757740020751953,
      "learning_rate": 1.4571948998178507e-07,
      "loss": 1.1033,
      "step": 32700
    },
    {
      "epoch": 1.9914996964177292,
      "grad_norm": 30.993314743041992,
      "learning_rate": 8.500303582270795e-08,
      "loss": 1.1176,
      "step": 32800
    },
    {
      "epoch": 1.997571341833637,
      "grad_norm": 12.030611038208008,
      "learning_rate": 2.428658166363085e-08,
      "loss": 1.0643,
      "step": 32900
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.466265320777893,
      "eval_runtime": 36.3862,
      "eval_samples_per_second": 333.478,
      "eval_steps_per_second": 41.692,
      "step": 32940
    }
  ],
  "logging_steps": 100,
  "max_steps": 32940,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2928736544417792e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
