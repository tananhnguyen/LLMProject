{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 16470,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006071645415907711,
      "grad_norm": 17.296768188476562,
      "learning_rate": 1.9939283545840924e-05,
      "loss": 4.4972,
      "step": 100
    },
    {
      "epoch": 0.012143290831815421,
      "grad_norm": 13.71591854095459,
      "learning_rate": 1.987856709168185e-05,
      "loss": 2.9394,
      "step": 200
    },
    {
      "epoch": 0.018214936247723135,
      "grad_norm": 13.020685195922852,
      "learning_rate": 1.9817850637522772e-05,
      "loss": 2.5704,
      "step": 300
    },
    {
      "epoch": 0.024286581663630843,
      "grad_norm": 29.990793228149414,
      "learning_rate": 1.9757134183363694e-05,
      "loss": 2.4712,
      "step": 400
    },
    {
      "epoch": 0.030358227079538554,
      "grad_norm": 9.987337112426758,
      "learning_rate": 1.9696417729204616e-05,
      "loss": 2.3206,
      "step": 500
    },
    {
      "epoch": 0.03642987249544627,
      "grad_norm": 18.389711380004883,
      "learning_rate": 1.963570127504554e-05,
      "loss": 2.2644,
      "step": 600
    },
    {
      "epoch": 0.042501517911353974,
      "grad_norm": 30.271087646484375,
      "learning_rate": 1.957498482088646e-05,
      "loss": 2.1997,
      "step": 700
    },
    {
      "epoch": 0.048573163327261686,
      "grad_norm": 15.486847877502441,
      "learning_rate": 1.9514268366727383e-05,
      "loss": 2.0972,
      "step": 800
    },
    {
      "epoch": 0.0546448087431694,
      "grad_norm": 11.138538360595703,
      "learning_rate": 1.9453551912568305e-05,
      "loss": 1.9894,
      "step": 900
    },
    {
      "epoch": 0.06071645415907711,
      "grad_norm": 15.592857360839844,
      "learning_rate": 1.939283545840923e-05,
      "loss": 2.0844,
      "step": 1000
    },
    {
      "epoch": 0.06678809957498483,
      "grad_norm": 14.83871841430664,
      "learning_rate": 1.9332119004250153e-05,
      "loss": 2.0147,
      "step": 1100
    },
    {
      "epoch": 0.07285974499089254,
      "grad_norm": 15.89639663696289,
      "learning_rate": 1.9271402550091076e-05,
      "loss": 1.8708,
      "step": 1200
    },
    {
      "epoch": 0.07893139040680024,
      "grad_norm": 37.4422607421875,
      "learning_rate": 1.9210686095931998e-05,
      "loss": 1.8113,
      "step": 1300
    },
    {
      "epoch": 0.08500303582270795,
      "grad_norm": 23.993112564086914,
      "learning_rate": 1.9149969641772923e-05,
      "loss": 1.958,
      "step": 1400
    },
    {
      "epoch": 0.09107468123861566,
      "grad_norm": 19.340011596679688,
      "learning_rate": 1.9089253187613846e-05,
      "loss": 1.8538,
      "step": 1500
    },
    {
      "epoch": 0.09714632665452337,
      "grad_norm": 28.554101943969727,
      "learning_rate": 1.9028536733454768e-05,
      "loss": 1.8256,
      "step": 1600
    },
    {
      "epoch": 0.10321797207043108,
      "grad_norm": 20.931018829345703,
      "learning_rate": 1.896782027929569e-05,
      "loss": 1.8387,
      "step": 1700
    },
    {
      "epoch": 0.1092896174863388,
      "grad_norm": 16.60072135925293,
      "learning_rate": 1.8907103825136616e-05,
      "loss": 1.7069,
      "step": 1800
    },
    {
      "epoch": 0.1153612629022465,
      "grad_norm": 20.90943145751953,
      "learning_rate": 1.8846387370977538e-05,
      "loss": 1.803,
      "step": 1900
    },
    {
      "epoch": 0.12143290831815422,
      "grad_norm": 13.064579010009766,
      "learning_rate": 1.878567091681846e-05,
      "loss": 1.7229,
      "step": 2000
    },
    {
      "epoch": 0.12750455373406194,
      "grad_norm": 25.447494506835938,
      "learning_rate": 1.8724954462659383e-05,
      "loss": 1.7588,
      "step": 2100
    },
    {
      "epoch": 0.13357619914996965,
      "grad_norm": 31.68619728088379,
      "learning_rate": 1.8664238008500305e-05,
      "loss": 1.735,
      "step": 2200
    },
    {
      "epoch": 0.13964784456587737,
      "grad_norm": 25.392379760742188,
      "learning_rate": 1.8603521554341227e-05,
      "loss": 1.673,
      "step": 2300
    },
    {
      "epoch": 0.14571948998178508,
      "grad_norm": 18.82859992980957,
      "learning_rate": 1.854280510018215e-05,
      "loss": 1.6506,
      "step": 2400
    },
    {
      "epoch": 0.15179113539769276,
      "grad_norm": 18.23980140686035,
      "learning_rate": 1.848208864602307e-05,
      "loss": 1.7038,
      "step": 2500
    },
    {
      "epoch": 0.15786278081360047,
      "grad_norm": 17.03887176513672,
      "learning_rate": 1.8421372191863997e-05,
      "loss": 1.7405,
      "step": 2600
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 16.816585540771484,
      "learning_rate": 1.836065573770492e-05,
      "loss": 1.6187,
      "step": 2700
    },
    {
      "epoch": 0.1700060716454159,
      "grad_norm": 23.315940856933594,
      "learning_rate": 1.8299939283545842e-05,
      "loss": 1.6822,
      "step": 2800
    },
    {
      "epoch": 0.1760777170613236,
      "grad_norm": 15.140145301818848,
      "learning_rate": 1.8239222829386764e-05,
      "loss": 1.6146,
      "step": 2900
    },
    {
      "epoch": 0.18214936247723132,
      "grad_norm": 34.49775314331055,
      "learning_rate": 1.817850637522769e-05,
      "loss": 1.7205,
      "step": 3000
    },
    {
      "epoch": 0.18822100789313903,
      "grad_norm": 17.49134635925293,
      "learning_rate": 1.8117789921068612e-05,
      "loss": 1.639,
      "step": 3100
    },
    {
      "epoch": 0.19429265330904674,
      "grad_norm": 31.088600158691406,
      "learning_rate": 1.8057073466909534e-05,
      "loss": 1.6492,
      "step": 3200
    },
    {
      "epoch": 0.20036429872495445,
      "grad_norm": 26.98057746887207,
      "learning_rate": 1.7996357012750456e-05,
      "loss": 1.5417,
      "step": 3300
    },
    {
      "epoch": 0.20643594414086217,
      "grad_norm": 31.30488395690918,
      "learning_rate": 1.7935640558591382e-05,
      "loss": 1.5661,
      "step": 3400
    },
    {
      "epoch": 0.21250758955676988,
      "grad_norm": 18.076705932617188,
      "learning_rate": 1.7874924104432304e-05,
      "loss": 1.6492,
      "step": 3500
    },
    {
      "epoch": 0.2185792349726776,
      "grad_norm": 34.4478874206543,
      "learning_rate": 1.7814207650273227e-05,
      "loss": 1.6886,
      "step": 3600
    },
    {
      "epoch": 0.2246508803885853,
      "grad_norm": 29.395824432373047,
      "learning_rate": 1.775349119611415e-05,
      "loss": 1.6072,
      "step": 3700
    },
    {
      "epoch": 0.230722525804493,
      "grad_norm": 27.109176635742188,
      "learning_rate": 1.769277474195507e-05,
      "loss": 1.6565,
      "step": 3800
    },
    {
      "epoch": 0.23679417122040072,
      "grad_norm": 18.217073440551758,
      "learning_rate": 1.7632058287795993e-05,
      "loss": 1.5788,
      "step": 3900
    },
    {
      "epoch": 0.24286581663630843,
      "grad_norm": 16.345178604125977,
      "learning_rate": 1.7571341833636916e-05,
      "loss": 1.5608,
      "step": 4000
    },
    {
      "epoch": 0.24893746205221615,
      "grad_norm": 24.3499698638916,
      "learning_rate": 1.7510625379477838e-05,
      "loss": 1.558,
      "step": 4100
    },
    {
      "epoch": 0.2550091074681239,
      "grad_norm": 12.699803352355957,
      "learning_rate": 1.7449908925318764e-05,
      "loss": 1.5126,
      "step": 4200
    },
    {
      "epoch": 0.2610807528840316,
      "grad_norm": 34.03799819946289,
      "learning_rate": 1.7389192471159686e-05,
      "loss": 1.5304,
      "step": 4300
    },
    {
      "epoch": 0.2671523982999393,
      "grad_norm": 22.707347869873047,
      "learning_rate": 1.7328476017000608e-05,
      "loss": 1.5529,
      "step": 4400
    },
    {
      "epoch": 0.273224043715847,
      "grad_norm": 30.899436950683594,
      "learning_rate": 1.726775956284153e-05,
      "loss": 1.5499,
      "step": 4500
    },
    {
      "epoch": 0.27929568913175473,
      "grad_norm": 43.488075256347656,
      "learning_rate": 1.7207043108682456e-05,
      "loss": 1.5391,
      "step": 4600
    },
    {
      "epoch": 0.28536733454766244,
      "grad_norm": 17.153024673461914,
      "learning_rate": 1.7146326654523378e-05,
      "loss": 1.5705,
      "step": 4700
    },
    {
      "epoch": 0.29143897996357016,
      "grad_norm": 13.521018028259277,
      "learning_rate": 1.70856102003643e-05,
      "loss": 1.6025,
      "step": 4800
    },
    {
      "epoch": 0.2975106253794778,
      "grad_norm": 20.22541046142578,
      "learning_rate": 1.7024893746205223e-05,
      "loss": 1.4878,
      "step": 4900
    },
    {
      "epoch": 0.3035822707953855,
      "grad_norm": 30.84392547607422,
      "learning_rate": 1.696417729204615e-05,
      "loss": 1.5974,
      "step": 5000
    },
    {
      "epoch": 0.30965391621129323,
      "grad_norm": 21.9709415435791,
      "learning_rate": 1.690346083788707e-05,
      "loss": 1.6044,
      "step": 5100
    },
    {
      "epoch": 0.31572556162720095,
      "grad_norm": 20.33124542236328,
      "learning_rate": 1.6842744383727993e-05,
      "loss": 1.4918,
      "step": 5200
    },
    {
      "epoch": 0.32179720704310866,
      "grad_norm": 20.368267059326172,
      "learning_rate": 1.6782027929568915e-05,
      "loss": 1.5605,
      "step": 5300
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 27.52159881591797,
      "learning_rate": 1.6721311475409837e-05,
      "loss": 1.4536,
      "step": 5400
    },
    {
      "epoch": 0.3339404978749241,
      "grad_norm": 34.00455856323242,
      "learning_rate": 1.666059502125076e-05,
      "loss": 1.5018,
      "step": 5500
    },
    {
      "epoch": 0.3400121432908318,
      "grad_norm": 7.465768814086914,
      "learning_rate": 1.6599878567091682e-05,
      "loss": 1.5123,
      "step": 5600
    },
    {
      "epoch": 0.3460837887067395,
      "grad_norm": 21.148021697998047,
      "learning_rate": 1.6539162112932604e-05,
      "loss": 1.4958,
      "step": 5700
    },
    {
      "epoch": 0.3521554341226472,
      "grad_norm": 17.867237091064453,
      "learning_rate": 1.647844565877353e-05,
      "loss": 1.5542,
      "step": 5800
    },
    {
      "epoch": 0.3582270795385549,
      "grad_norm": 16.910526275634766,
      "learning_rate": 1.6417729204614452e-05,
      "loss": 1.5247,
      "step": 5900
    },
    {
      "epoch": 0.36429872495446264,
      "grad_norm": 28.750675201416016,
      "learning_rate": 1.6357012750455374e-05,
      "loss": 1.5195,
      "step": 6000
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 27.628952026367188,
      "learning_rate": 1.6296296296296297e-05,
      "loss": 1.5842,
      "step": 6100
    },
    {
      "epoch": 0.37644201578627806,
      "grad_norm": 35.35530090332031,
      "learning_rate": 1.6235579842137222e-05,
      "loss": 1.4549,
      "step": 6200
    },
    {
      "epoch": 0.3825136612021858,
      "grad_norm": 19.119380950927734,
      "learning_rate": 1.6174863387978145e-05,
      "loss": 1.4935,
      "step": 6300
    },
    {
      "epoch": 0.3885853066180935,
      "grad_norm": 24.729158401489258,
      "learning_rate": 1.6114146933819067e-05,
      "loss": 1.5583,
      "step": 6400
    },
    {
      "epoch": 0.3946569520340012,
      "grad_norm": 14.224651336669922,
      "learning_rate": 1.605343047965999e-05,
      "loss": 1.5411,
      "step": 6500
    },
    {
      "epoch": 0.4007285974499089,
      "grad_norm": 22.519134521484375,
      "learning_rate": 1.599271402550091e-05,
      "loss": 1.5275,
      "step": 6600
    },
    {
      "epoch": 0.4068002428658166,
      "grad_norm": 18.582857131958008,
      "learning_rate": 1.5931997571341834e-05,
      "loss": 1.4711,
      "step": 6700
    },
    {
      "epoch": 0.41287188828172433,
      "grad_norm": 35.604835510253906,
      "learning_rate": 1.5871281117182756e-05,
      "loss": 1.4516,
      "step": 6800
    },
    {
      "epoch": 0.41894353369763204,
      "grad_norm": 25.927825927734375,
      "learning_rate": 1.5810564663023678e-05,
      "loss": 1.3899,
      "step": 6900
    },
    {
      "epoch": 0.42501517911353975,
      "grad_norm": 27.476083755493164,
      "learning_rate": 1.5749848208864604e-05,
      "loss": 1.4291,
      "step": 7000
    },
    {
      "epoch": 0.43108682452944747,
      "grad_norm": 27.45785140991211,
      "learning_rate": 1.5689131754705526e-05,
      "loss": 1.5117,
      "step": 7100
    },
    {
      "epoch": 0.4371584699453552,
      "grad_norm": 25.33167266845703,
      "learning_rate": 1.5628415300546448e-05,
      "loss": 1.4305,
      "step": 7200
    },
    {
      "epoch": 0.4432301153612629,
      "grad_norm": 28.904844284057617,
      "learning_rate": 1.556769884638737e-05,
      "loss": 1.4193,
      "step": 7300
    },
    {
      "epoch": 0.4493017607771706,
      "grad_norm": 28.714426040649414,
      "learning_rate": 1.5506982392228296e-05,
      "loss": 1.4477,
      "step": 7400
    },
    {
      "epoch": 0.4553734061930783,
      "grad_norm": 21.289949417114258,
      "learning_rate": 1.544626593806922e-05,
      "loss": 1.4044,
      "step": 7500
    },
    {
      "epoch": 0.461445051608986,
      "grad_norm": 35.0536994934082,
      "learning_rate": 1.538554948391014e-05,
      "loss": 1.4728,
      "step": 7600
    },
    {
      "epoch": 0.46751669702489373,
      "grad_norm": 16.52252769470215,
      "learning_rate": 1.5324833029751063e-05,
      "loss": 1.3961,
      "step": 7700
    },
    {
      "epoch": 0.47358834244080145,
      "grad_norm": 33.12568283081055,
      "learning_rate": 1.526411657559199e-05,
      "loss": 1.3734,
      "step": 7800
    },
    {
      "epoch": 0.47965998785670916,
      "grad_norm": 25.07231903076172,
      "learning_rate": 1.5203400121432909e-05,
      "loss": 1.4311,
      "step": 7900
    },
    {
      "epoch": 0.48573163327261687,
      "grad_norm": 43.82021713256836,
      "learning_rate": 1.5142683667273831e-05,
      "loss": 1.4833,
      "step": 8000
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 16.152624130249023,
      "learning_rate": 1.5081967213114754e-05,
      "loss": 1.4357,
      "step": 8100
    },
    {
      "epoch": 0.4978749241044323,
      "grad_norm": 25.476961135864258,
      "learning_rate": 1.502125075895568e-05,
      "loss": 1.38,
      "step": 8200
    },
    {
      "epoch": 0.50394656952034,
      "grad_norm": 19.551677703857422,
      "learning_rate": 1.4960534304796602e-05,
      "loss": 1.3712,
      "step": 8300
    },
    {
      "epoch": 0.5100182149362478,
      "grad_norm": 20.78053855895996,
      "learning_rate": 1.4899817850637524e-05,
      "loss": 1.3842,
      "step": 8400
    },
    {
      "epoch": 0.5160898603521554,
      "grad_norm": 8.279583930969238,
      "learning_rate": 1.4839101396478446e-05,
      "loss": 1.4708,
      "step": 8500
    },
    {
      "epoch": 0.5221615057680632,
      "grad_norm": 21.942625045776367,
      "learning_rate": 1.477838494231937e-05,
      "loss": 1.4792,
      "step": 8600
    },
    {
      "epoch": 0.5282331511839709,
      "grad_norm": 13.500702857971191,
      "learning_rate": 1.4717668488160292e-05,
      "loss": 1.4262,
      "step": 8700
    },
    {
      "epoch": 0.5343047965998786,
      "grad_norm": 19.362268447875977,
      "learning_rate": 1.4656952034001215e-05,
      "loss": 1.5097,
      "step": 8800
    },
    {
      "epoch": 0.5403764420157863,
      "grad_norm": 16.131145477294922,
      "learning_rate": 1.4596235579842137e-05,
      "loss": 1.4532,
      "step": 8900
    },
    {
      "epoch": 0.546448087431694,
      "grad_norm": 17.23655891418457,
      "learning_rate": 1.4535519125683062e-05,
      "loss": 1.4328,
      "step": 9000
    },
    {
      "epoch": 0.5525197328476017,
      "grad_norm": 31.24182891845703,
      "learning_rate": 1.4474802671523985e-05,
      "loss": 1.4773,
      "step": 9100
    },
    {
      "epoch": 0.5585913782635095,
      "grad_norm": 19.929569244384766,
      "learning_rate": 1.4414086217364907e-05,
      "loss": 1.4415,
      "step": 9200
    },
    {
      "epoch": 0.5646630236794171,
      "grad_norm": 16.804668426513672,
      "learning_rate": 1.435336976320583e-05,
      "loss": 1.3762,
      "step": 9300
    },
    {
      "epoch": 0.5707346690953249,
      "grad_norm": 19.582782745361328,
      "learning_rate": 1.4292653309046753e-05,
      "loss": 1.4189,
      "step": 9400
    },
    {
      "epoch": 0.5768063145112325,
      "grad_norm": 15.05738639831543,
      "learning_rate": 1.4231936854887675e-05,
      "loss": 1.3335,
      "step": 9500
    },
    {
      "epoch": 0.5828779599271403,
      "grad_norm": 14.130531311035156,
      "learning_rate": 1.4171220400728598e-05,
      "loss": 1.4085,
      "step": 9600
    },
    {
      "epoch": 0.588949605343048,
      "grad_norm": 18.10894775390625,
      "learning_rate": 1.4110503946569523e-05,
      "loss": 1.4402,
      "step": 9700
    },
    {
      "epoch": 0.5950212507589556,
      "grad_norm": 18.544111251831055,
      "learning_rate": 1.4049787492410446e-05,
      "loss": 1.2679,
      "step": 9800
    },
    {
      "epoch": 0.6010928961748634,
      "grad_norm": 12.670514106750488,
      "learning_rate": 1.3989071038251368e-05,
      "loss": 1.3216,
      "step": 9900
    },
    {
      "epoch": 0.607164541590771,
      "grad_norm": 16.284801483154297,
      "learning_rate": 1.392835458409229e-05,
      "loss": 1.3396,
      "step": 10000
    },
    {
      "epoch": 0.6132361870066788,
      "grad_norm": 41.70616149902344,
      "learning_rate": 1.3867638129933214e-05,
      "loss": 1.38,
      "step": 10100
    },
    {
      "epoch": 0.6193078324225865,
      "grad_norm": 6.528571128845215,
      "learning_rate": 1.3806921675774136e-05,
      "loss": 1.426,
      "step": 10200
    },
    {
      "epoch": 0.6253794778384942,
      "grad_norm": 53.03622817993164,
      "learning_rate": 1.3746205221615059e-05,
      "loss": 1.3747,
      "step": 10300
    },
    {
      "epoch": 0.6314511232544019,
      "grad_norm": 9.3684720993042,
      "learning_rate": 1.368548876745598e-05,
      "loss": 1.3717,
      "step": 10400
    },
    {
      "epoch": 0.6375227686703097,
      "grad_norm": 21.23341178894043,
      "learning_rate": 1.3624772313296906e-05,
      "loss": 1.2517,
      "step": 10500
    },
    {
      "epoch": 0.6435944140862173,
      "grad_norm": 21.24834632873535,
      "learning_rate": 1.3564055859137829e-05,
      "loss": 1.3545,
      "step": 10600
    },
    {
      "epoch": 0.6496660595021251,
      "grad_norm": 12.156889915466309,
      "learning_rate": 1.3503339404978751e-05,
      "loss": 1.3879,
      "step": 10700
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 28.569732666015625,
      "learning_rate": 1.3442622950819673e-05,
      "loss": 1.3646,
      "step": 10800
    },
    {
      "epoch": 0.6618093503339405,
      "grad_norm": 18.82826805114746,
      "learning_rate": 1.3381906496660597e-05,
      "loss": 1.4716,
      "step": 10900
    },
    {
      "epoch": 0.6678809957498482,
      "grad_norm": 20.62760353088379,
      "learning_rate": 1.332119004250152e-05,
      "loss": 1.3442,
      "step": 11000
    },
    {
      "epoch": 0.6739526411657559,
      "grad_norm": 22.21361541748047,
      "learning_rate": 1.3260473588342442e-05,
      "loss": 1.371,
      "step": 11100
    },
    {
      "epoch": 0.6800242865816636,
      "grad_norm": 21.08370018005371,
      "learning_rate": 1.3199757134183364e-05,
      "loss": 1.3984,
      "step": 11200
    },
    {
      "epoch": 0.6860959319975714,
      "grad_norm": 34.676963806152344,
      "learning_rate": 1.3139040680024288e-05,
      "loss": 1.4646,
      "step": 11300
    },
    {
      "epoch": 0.692167577413479,
      "grad_norm": 11.107781410217285,
      "learning_rate": 1.307832422586521e-05,
      "loss": 1.3988,
      "step": 11400
    },
    {
      "epoch": 0.6982392228293868,
      "grad_norm": 27.688982009887695,
      "learning_rate": 1.3017607771706134e-05,
      "loss": 1.3226,
      "step": 11500
    },
    {
      "epoch": 0.7043108682452944,
      "grad_norm": 23.969987869262695,
      "learning_rate": 1.2956891317547056e-05,
      "loss": 1.3347,
      "step": 11600
    },
    {
      "epoch": 0.7103825136612022,
      "grad_norm": 23.36321258544922,
      "learning_rate": 1.289617486338798e-05,
      "loss": 1.4263,
      "step": 11700
    },
    {
      "epoch": 0.7164541590771099,
      "grad_norm": 14.796408653259277,
      "learning_rate": 1.2835458409228903e-05,
      "loss": 1.3012,
      "step": 11800
    },
    {
      "epoch": 0.7225258044930176,
      "grad_norm": 37.18631362915039,
      "learning_rate": 1.2774741955069825e-05,
      "loss": 1.3099,
      "step": 11900
    },
    {
      "epoch": 0.7285974499089253,
      "grad_norm": 13.765080451965332,
      "learning_rate": 1.2714025500910747e-05,
      "loss": 1.3394,
      "step": 12000
    },
    {
      "epoch": 0.734669095324833,
      "grad_norm": 64.35848999023438,
      "learning_rate": 1.2653309046751671e-05,
      "loss": 1.3431,
      "step": 12100
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 39.097923278808594,
      "learning_rate": 1.2592592592592593e-05,
      "loss": 1.3785,
      "step": 12200
    },
    {
      "epoch": 0.7468123861566485,
      "grad_norm": 31.854198455810547,
      "learning_rate": 1.2531876138433516e-05,
      "loss": 1.3521,
      "step": 12300
    },
    {
      "epoch": 0.7528840315725561,
      "grad_norm": 12.823427200317383,
      "learning_rate": 1.247115968427444e-05,
      "loss": 1.4084,
      "step": 12400
    },
    {
      "epoch": 0.7589556769884639,
      "grad_norm": 14.020084381103516,
      "learning_rate": 1.2410443230115363e-05,
      "loss": 1.3385,
      "step": 12500
    },
    {
      "epoch": 0.7650273224043715,
      "grad_norm": 20.53643798828125,
      "learning_rate": 1.2349726775956286e-05,
      "loss": 1.328,
      "step": 12600
    },
    {
      "epoch": 0.7710989678202793,
      "grad_norm": 24.135656356811523,
      "learning_rate": 1.2289010321797208e-05,
      "loss": 1.3866,
      "step": 12700
    },
    {
      "epoch": 0.777170613236187,
      "grad_norm": 27.215164184570312,
      "learning_rate": 1.222829386763813e-05,
      "loss": 1.3133,
      "step": 12800
    },
    {
      "epoch": 0.7832422586520947,
      "grad_norm": 32.85581970214844,
      "learning_rate": 1.2167577413479054e-05,
      "loss": 1.4234,
      "step": 12900
    },
    {
      "epoch": 0.7893139040680024,
      "grad_norm": 45.45546340942383,
      "learning_rate": 1.2106860959319976e-05,
      "loss": 1.3502,
      "step": 13000
    },
    {
      "epoch": 0.7953855494839102,
      "grad_norm": 3.866055965423584,
      "learning_rate": 1.2046144505160899e-05,
      "loss": 1.3274,
      "step": 13100
    },
    {
      "epoch": 0.8014571948998178,
      "grad_norm": 30.376605987548828,
      "learning_rate": 1.1985428051001821e-05,
      "loss": 1.283,
      "step": 13200
    },
    {
      "epoch": 0.8075288403157256,
      "grad_norm": 17.974870681762695,
      "learning_rate": 1.1924711596842747e-05,
      "loss": 1.3729,
      "step": 13300
    },
    {
      "epoch": 0.8136004857316332,
      "grad_norm": 17.81493377685547,
      "learning_rate": 1.1863995142683669e-05,
      "loss": 1.3555,
      "step": 13400
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 32.74492645263672,
      "learning_rate": 1.1803278688524591e-05,
      "loss": 1.3436,
      "step": 13500
    },
    {
      "epoch": 0.8257437765634487,
      "grad_norm": 22.810827255249023,
      "learning_rate": 1.1742562234365513e-05,
      "loss": 1.3605,
      "step": 13600
    },
    {
      "epoch": 0.8318154219793564,
      "grad_norm": 47.99007797241211,
      "learning_rate": 1.1681845780206437e-05,
      "loss": 1.2779,
      "step": 13700
    },
    {
      "epoch": 0.8378870673952641,
      "grad_norm": 20.353296279907227,
      "learning_rate": 1.162112932604736e-05,
      "loss": 1.4123,
      "step": 13800
    },
    {
      "epoch": 0.8439587128111719,
      "grad_norm": 17.272293090820312,
      "learning_rate": 1.1560412871888282e-05,
      "loss": 1.3472,
      "step": 13900
    },
    {
      "epoch": 0.8500303582270795,
      "grad_norm": 15.608070373535156,
      "learning_rate": 1.1499696417729204e-05,
      "loss": 1.3289,
      "step": 14000
    },
    {
      "epoch": 0.8561020036429873,
      "grad_norm": 16.816646575927734,
      "learning_rate": 1.143897996357013e-05,
      "loss": 1.3302,
      "step": 14100
    },
    {
      "epoch": 0.8621736490588949,
      "grad_norm": 16.382713317871094,
      "learning_rate": 1.1378263509411052e-05,
      "loss": 1.3309,
      "step": 14200
    },
    {
      "epoch": 0.8682452944748027,
      "grad_norm": 11.935954093933105,
      "learning_rate": 1.1317547055251974e-05,
      "loss": 1.3653,
      "step": 14300
    },
    {
      "epoch": 0.8743169398907104,
      "grad_norm": 20.81771469116211,
      "learning_rate": 1.1256830601092897e-05,
      "loss": 1.3699,
      "step": 14400
    },
    {
      "epoch": 0.8803885853066181,
      "grad_norm": 34.74507141113281,
      "learning_rate": 1.119611414693382e-05,
      "loss": 1.3907,
      "step": 14500
    },
    {
      "epoch": 0.8864602307225258,
      "grad_norm": 20.780372619628906,
      "learning_rate": 1.1135397692774743e-05,
      "loss": 1.3322,
      "step": 14600
    },
    {
      "epoch": 0.8925318761384335,
      "grad_norm": 20.61725425720215,
      "learning_rate": 1.1074681238615665e-05,
      "loss": 1.3806,
      "step": 14700
    },
    {
      "epoch": 0.8986035215543412,
      "grad_norm": 29.264556884765625,
      "learning_rate": 1.1013964784456587e-05,
      "loss": 1.3796,
      "step": 14800
    },
    {
      "epoch": 0.904675166970249,
      "grad_norm": 25.27979850769043,
      "learning_rate": 1.0953248330297513e-05,
      "loss": 1.343,
      "step": 14900
    },
    {
      "epoch": 0.9107468123861566,
      "grad_norm": 15.622150421142578,
      "learning_rate": 1.0892531876138435e-05,
      "loss": 1.3413,
      "step": 15000
    },
    {
      "epoch": 0.9168184578020644,
      "grad_norm": 17.76564598083496,
      "learning_rate": 1.0831815421979357e-05,
      "loss": 1.3076,
      "step": 15100
    },
    {
      "epoch": 0.922890103217972,
      "grad_norm": 10.593975067138672,
      "learning_rate": 1.077109896782028e-05,
      "loss": 1.2804,
      "step": 15200
    },
    {
      "epoch": 0.9289617486338798,
      "grad_norm": 34.68490982055664,
      "learning_rate": 1.0710382513661204e-05,
      "loss": 1.3346,
      "step": 15300
    },
    {
      "epoch": 0.9350333940497875,
      "grad_norm": 37.239723205566406,
      "learning_rate": 1.0649666059502126e-05,
      "loss": 1.3117,
      "step": 15400
    },
    {
      "epoch": 0.9411050394656952,
      "grad_norm": 20.66153335571289,
      "learning_rate": 1.0588949605343048e-05,
      "loss": 1.3195,
      "step": 15500
    },
    {
      "epoch": 0.9471766848816029,
      "grad_norm": 23.84140968322754,
      "learning_rate": 1.052823315118397e-05,
      "loss": 1.2573,
      "step": 15600
    },
    {
      "epoch": 0.9532483302975107,
      "grad_norm": 22.295421600341797,
      "learning_rate": 1.0467516697024896e-05,
      "loss": 1.3242,
      "step": 15700
    },
    {
      "epoch": 0.9593199757134183,
      "grad_norm": 52.20244598388672,
      "learning_rate": 1.0406800242865818e-05,
      "loss": 1.405,
      "step": 15800
    },
    {
      "epoch": 0.9653916211293261,
      "grad_norm": 53.490882873535156,
      "learning_rate": 1.034608378870674e-05,
      "loss": 1.3206,
      "step": 15900
    },
    {
      "epoch": 0.9714632665452337,
      "grad_norm": 33.75416564941406,
      "learning_rate": 1.0285367334547663e-05,
      "loss": 1.4452,
      "step": 16000
    },
    {
      "epoch": 0.9775349119611415,
      "grad_norm": 22.605735778808594,
      "learning_rate": 1.0224650880388587e-05,
      "loss": 1.3662,
      "step": 16100
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 18.9329776763916,
      "learning_rate": 1.0163934426229509e-05,
      "loss": 1.2806,
      "step": 16200
    },
    {
      "epoch": 0.9896782027929569,
      "grad_norm": 16.06412124633789,
      "learning_rate": 1.0103217972070431e-05,
      "loss": 1.2662,
      "step": 16300
    },
    {
      "epoch": 0.9957498482088646,
      "grad_norm": 17.541458129882812,
      "learning_rate": 1.0042501517911354e-05,
      "loss": 1.2533,
      "step": 16400
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.4866690635681152,
      "eval_runtime": 36.9733,
      "eval_samples_per_second": 328.183,
      "eval_steps_per_second": 41.03,
      "step": 16470
    }
  ],
  "logging_steps": 100,
  "max_steps": 32940,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6464368272208896.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
